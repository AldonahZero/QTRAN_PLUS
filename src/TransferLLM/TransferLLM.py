"""
è½¬æ¢é˜¶æ®µæ ¸å¿ƒï¼šåŸºäº LLM çš„è·¨æ–¹è¨€ SQL è½¬æ¢ä¸é”™è¯¯è¿­ä»£ä¿®æ­£

ä½œç”¨æ¦‚è¿°ï¼š
- æ¥æ”¶æ¥æº SQLï¼ˆå¦‚æ¥è‡ª sqlancer/pinoloï¼‰ï¼Œç»“åˆ Few-Shot ç¤ºä¾‹ä¸æ–¹è¨€ç‰¹å¾çŸ¥è¯†åº“ï¼Œè°ƒç”¨ LLM ç”Ÿæˆç›®æ ‡æ•°æ®åº“å¯æ‰§è¡Œ SQLã€‚
- è‹¥æ‰§è¡Œå¤±è´¥ï¼Œåˆ©ç”¨é”™è¯¯ä¿¡æ¯è¿›è¡Œè‹¥å¹²æ¬¡è¿­ä»£ä¿®æ­£ï¼Œç›´åˆ°æˆåŠŸæˆ–è¾¾ä¸Šé™ï¼›å…¨ç¨‹è®°å½•æ¶ˆè€—ã€ç»“æœä¸é”™è¯¯ã€‚
- æä¾›æ•°æ®åŠ è½½/åˆå§‹åŒ–å·¥å…·ï¼ˆpinolo æ•°æ®ï¼‰ä¸åˆ—åé¢„å¤„ç†ç­‰è¾…åŠ©èƒ½åŠ›ã€‚

å…³è”æµç¨‹å‚è€ƒï¼šè§ abstract.mdã€Šé˜¶æ®µä¸€ï¼šè½¬æ¢ (Transfer Phase)ã€‹ã€Šè°ƒç”¨é“¾æ¦‚è§ˆã€‹ä¸­çš„ transfer_llmã€‚
"""

# !/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2024/7/26 17:09
# @Author  : huanghe
# @File    : TransferLLM.py
# @Intro   :

import os
import json
import random
import time
import re
from typing import Optional, Dict, Any
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import ResponseSchema
from langchain.output_parsers import StructuredOutputParser
from langchain.callbacks import get_openai_callback
from src.Tools.DatabaseConnect.database_connector import exec_sql_statement
from src.NoSQLFuzz.nosql_crash_pipeline import run_nosql_sequence
from src.Tools.json_utils import safe_parse_result

# Optional: Redis KB adapter for prompt augmentation (lazy import)
try:
    from src.NoSQLKnowledgeBaseConstruction.Redis.redis_kb_adapter import (
        select_redis_candidates,
        build_prompt_with_kb,
    )

    _REDIS_KB_AVAILABLE = True
except Exception:
    _REDIS_KB_AVAILABLE = False


db_names = ["mysql", "mariadb", "tidb"]


def load_data(output_name, db_name, len_low, len_high, is_random, num):
    """
    åŠ è½½ pinolo åŸå§‹ SQL åŠ SQLsim æ•°æ®ï¼Œå¹¶æŒ‰é•¿åº¦ä¸æ˜¯å¦éšæœºæŠ½æ ·å¯¼å‡ºåˆ°è¾“å‡ºé›†ã€‚

    å‚æ•°ï¼š
    - output_name/db_name: æ•°æ®æºç›®å½•å®šä½ã€‚
    - len_low/len_high: è¿‡æ»¤é•¿åº¦ [low, high)ã€‚
    - is_random/num: éšæœºæŠ½æ ·ä¸æŠ½æ ·æ•°ã€‚

    è¯´æ˜ï¼šä¸ä¼šä¿®æ”¹ä¸šåŠ¡é€»è¾‘ï¼Œä»…æ•´ç†å¹¶æŒä¹…åŒ–ç­›é€‰åçš„ SQL/SQLsim ä¸å…¶ç´¢å¼•ã€‚
    """
    # åŠ è½½æ•°æ®ï¼š
    # åŠ è½½æ‰€æœ‰çš„originalSqlåŠå¯¹åº”çš„originalSqlsimæ•°æ®ï¼šé•¿åº¦åœ¨[len_low,len_high)ã€‚
    # len_high = float('inf')ä»£è¡¨æ— ç©·å¤§;len_low=1,len_high = float('inf')åˆ™è¡¨ç¤ºè·å–æ‰€æœ‰æ•°æ®
    # random=Trueæ—¶,åˆ™éšæœºé€‰æ‹©numæ¡æ•°æ®ï¼›random=Falseæ—¶ï¼Œåˆ™è¿”å›é•¿åº¦åœ¨[len_low,len_high)çš„æ‰€æœ‰æ•°æ®
    # æºæ–‡ä»¶ï¼šPinolo Output/output(1-4)/dbnameæ–‡ä»¶å¤¹å†…çš„originalSql_all.json,originalSqlsim_all.json
    # ç›®æ ‡æ–‡ä»¶ï¼šPinolo Output/output_testä¸‹ALLå’ŒRANDOMæ–‡ä»¶å¤¹å†…çš„output1_mariadb_x_x_originalSql_all.jsonï¼Œoutput1_mariadb_x_x_originalSqlsim_all.jsonï¼Œoutput1_mariadb_x_x_originalSqlIndex_all.json
    if len_low >= len_high:
        return
    # sqlè¯­å¥çš„æ–‡ä»¶å
    filename = os.path.join(
        "..\..\Dataset\Pinolo Output",
        output_name,
        db_name + "_merged",
        "originalSql_all.json",
    )
    # sqlsimè¯­å¥çš„æ–‡ä»¶å
    sim_filename = os.path.join(
        "..\..\Dataset\Pinolo Output",
        output_name,
        db_name + "_merged",
        "originalSqlsim_all.json",
    )
    # å­˜å‚¨åŠ è½½æ‰€å¾—æ•°æ®çš„ç›®æ ‡æ–‡ä»¶å
    dir_filename = os.path.join(
        "..\..\Dataset\Pinolo Output",
        "output_test",
        "ALL",
        output_name
        + "_"
        + db_name
        + "_"
        + str(len_low)
        + "_"
        + str(len_high)
        + "_originalSql_all.json",
    )
    dir_sim_filename = os.path.join(
        "..\..\Dataset\Pinolo Output",
        "output_test",
        "ALL",
        output_name
        + "_"
        + db_name
        + "_"
        + str(len_low)
        + "_"
        + str(len_high)
        + "_originalSqlsim_all.json",
    )
    dir_index_filename = os.path.join(
        "..\..\Dataset\Pinolo Output",
        "output_test",
        "ALL",
        output_name
        + "_"
        + db_name
        + "_"
        + str(len_low)
        + "_"
        + str(len_high)
        + "_originalSqlIndex_all.json",
    )

    # æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿”å›
    if (
        os.path.exists(dir_filename)
        and os.path.exists(dir_sim_filename)
        and os.path.exists(dir_index_filename)
    ):
        return

    with open(filename, "r", encoding="utf-8") as rf:
        contents = json.load(rf)
    with open(sim_filename, "r", encoding="utf-8") as f:
        sim_contents = json.load(f)

    selected_sqls = []
    selected_sqlsims = []
    selected_sql_indexes = []
    # é€‰å–æŒ‡å®šé•¿åº¦åŒºé—´çš„æ•°æ®
    for i in range(len(contents)):
        content = contents[i]
        if len_low <= len(content) < len_high:
            selected_sqls.append(content)
            selected_sqlsims.append(sim_contents[i])
            selected_sql_indexes.append(i)

    if is_random:
        # éšæœºåŠ è½½ï¼šä»æŒ‡å®šé•¿åº¦åŒºé—´ä¸­éšæœºé€‰æ‹©numæ¡æ•°æ®å¹¶è¿”å›
        random_num = 0
        random_sqls = []
        random_sqlsims = []
        random_sql_indexes = []
        if len(selected_sqls) < num:
            # [len_low,len_high)çš„æ•°é‡å°äºè¦æå–çš„æ•°ç›®num
            random_num = len(selected_sqls)
            random_sqls = selected_sqls
            random_sqlsims = selected_sqlsims
            random_sql_indexes = selected_sql_indexes
        else:
            random_num = num
            random_numbers = random.sample(range(0, len(selected_sqls)), num)
            for number in random_numbers:
                random_sqls.append(selected_sqls[number])
                random_sqlsims.append(selected_sqlsims[number])
                random_sql_indexes.append(selected_sql_indexes[number])
        random_dir_filename = os.path.join(
            "..\..\Dataset\Pinolo Output",
            "output_test",
            "RANDOM",
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSql_random_"
            + str(random_num)
            + ".json",
        )
        random_dir_sim_filename = os.path.join(
            "..\..\Dataset\Pinolo Output",
            "output_test",
            "RANDOM",
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlsim_random_"
            + str(random_num)
            + ".json",
        )
        random_dir_index_filename = os.path.join(
            "..\..\Dataset\Pinolo Output",
            "output_test",
            "RANDOM",
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlIndex_ramdom_"
            + str(random_num)
            + ".json",
        )
        with open(random_dir_filename, "w", encoding="utf-8") as f:
            json.dump(random_sqls, f, indent=4)
        with open(random_dir_sim_filename, "w", encoding="utf-8") as f:
            json.dump(random_sqlsims, f, indent=4)
        with open(random_dir_index_filename, "w", encoding="utf-8") as f:
            json.dump(random_sql_indexes, f, indent=4)
        print("ğŸ“¥ " + random_dir_filename + ":" + str(len(random_sqls)))
        print("ğŸ“¥ " + random_dir_sim_filename + ":" + str(len(random_sqlsims)))
    else:
        # å…¨éƒ¨åŠ è½½ï¼šç›´æ¥å­˜å‚¨æ‰€æœ‰[len_low,len_high)é•¿åº¦çš„æ•°æ®
        with open(dir_filename, "w", encoding="utf-8") as f:
            json.dump(selected_sqls, f, indent=4)
        with open(dir_sim_filename, "w", encoding="utf-8") as f:
            json.dump(selected_sqlsims, f, indent=4)
        with open(dir_index_filename, "w", encoding="utf-8") as f:
            json.dump(selected_sql_indexes, f, indent=4)

        print("ğŸ“¥ " + dir_filename + ":" + str(len(selected_sqls)))
        print("ğŸ“¥ " + dir_sim_filename + ":" + str(len(selected_sqlsims)))


def init_data(output_name, db_name, len_low, len_high, is_random, num):
    """
    # åˆå§‹åŒ–ç»“æœæ•°æ®ï¼š
    # æ ¹æ®load_data()å¾—åˆ°çš„æ•°æ®ï¼Œåˆå§‹åŒ–ç»“æœæ•°æ®å¹¶å­˜å‚¨åˆ°output_testæ–‡ä»¶å¤¹ä¸­
    # æºæ–‡ä»¶ï¼šPinolo Output/output_testä¸‹ALLå’ŒRANDOMæ–‡ä»¶å¤¹å†…çš„output1_mariadb_x_x_originalSql_all.jsonï¼Œoutput1_mariadb_x_x_originalSqlsim_all.jsonï¼Œoutput1_mariadb_x_x_originalSqlIndex_all.json
    # ç›®æ ‡æ–‡ä»¶ï¼šPinolo Output/output_testä¸‹ALLå’ŒRANDOMæ–‡ä»¶å¤¹å†…çš„init_output1_mariadb_x_x_originalSql_all.json,init_output1_mariadb_x_x_originalSqlsim_all.json
    """
    prefix = os.path.join("..\..\Dataset\Pinolo Output", "output_test")
    # exec_args = database_connection_args[db_name]

    # æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶
    if is_random:
        sql_temp = (
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSql_random_"
            + str(num)
            + ".json"
        )
        sqlsim_temp = (
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlsim_random_"
            + str(num)
            + ".json"
        )
        index_temp = (
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlIndex_ramdom_"
            + str(num)
            + ".json"
        )
        sql_filename = os.path.join(prefix, "RANDOM", sql_temp)
        sqlsim_filename = os.path.join(prefix, "RANDOM", sqlsim_temp)
        index_filename = os.path.join(prefix, "RANDOM", index_temp)
        sql_dir_filename = os.path.join(prefix, "RANDOM", "init_" + sql_temp)
        sqlsim_dir_filename = os.path.join(prefix, "RANDOM", "init_" + sqlsim_temp)
    else:
        sql_temp = (
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSql_all.json"
        )
        sqlsim_temp = (
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlsim_all.json"
        )
        index_temp = (
            output_name
            + "_"
            + db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlIndex_all.json"
        )
        sql_filename = os.path.join(prefix, "ALL", sql_temp)
        sqlsim_filename = os.path.join(prefix, "ALL", sqlsim_temp)
        index_filename = os.path.join(prefix, "ALL", index_temp)
        sql_dir_filename = os.path.join(prefix, "ALL", "init_" + sql_temp)
        sqlsim_dir_filename = os.path.join(prefix, "ALL", "init_" + sqlsim_temp)

    # æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¿”å›
    if os.path.exists(sql_dir_filename) and os.path.exists(sqlsim_dir_filename):
        return

    with open(sql_filename, "r", encoding="utf-8") as rf:
        sqls = json.load(rf)
    with open(sqlsim_filename, "r", encoding="utf-8") as rf:
        sqlsims = json.load(rf)
    with open(index_filename, "r", encoding="utf-8") as rf:
        sql_indexes = json.load(rf)
    # åˆå§‹åŒ–ç»“æœæ•°æ®
    sql_init_results = []
    sqlsim_init_results = []
    avg_sql_length = 0
    avg_sqlsim_length = 0
    for index in range(len(sqls)):
        result = {
            "index": index,
            "origin_index": sql_indexes[index],
            "Sql": sqls[index],
            "SqlLength": len(sqls[index]),
            "SqlExecResult": "",
            "SqlExecTime": "",
            "SqlExecError": "",
            "TransferResult": [],
            "TransferCost": [],
            "TransferSqlExecResult": [],
            "TransferSqlExecTime": [],
            "TransferSqlExecError": [],
            "TransferSqlExecEqualities": [],
        }
        sim_result = {
            "index": index,
            "origin_index": sql_indexes[index],
            "Sql": sqlsims[index],
            "SqlLength": len(sqlsims[index]),
            "SqlExecResult": "",
            "SqlExecTime": "",
            "SqlExecError": "",
            "TransferResult": [],
            "TransferCost": [],
            "TransferSqlExecResult": [],
            "TransferSqlExecTime": [],
            "TransferSqlExecError": [],
            "TransferSqlExecEqualities": [],
        }
        avg_sql_length += len(sqls[index])
        avg_sqlsim_length += len(sqlsims[index])
        sql_init_results.append(result)
        sqlsim_init_results.append(sim_result)

    avg_sql_length /= len(sqls)
    avg_sqlsim_length /= len(sqlsims)
    print("sqlå¹³å‡é•¿åº¦ï¼š" + str(avg_sql_length))
    print("sqlsimå¹³å‡é•¿åº¦ï¼š" + str(avg_sqlsim_length))
    with open(sql_dir_filename, "w", encoding="utf-8") as f:
        json.dump(sql_init_results, f, indent=4)

    with open(sqlsim_dir_filename, "w", encoding="utf-8") as f:
        json.dump(sqlsim_init_results, f, indent=4)


# å¤„ç†ä¼ é€’ç»™transfer llmçš„sql statement(è¿™æ˜¯é’ˆå¯¹pinoloçš„process)
def sql_statement_process(origin_sql):
    """Pinolo åˆ° Postgres å‰çš„åˆ—åé¢„å¤„ç†ï¼šç»Ÿä¸€æŸäº›ç‰¹æ®Šåˆ—åå†™æ³•ï¼Œä¾¿äºåç»­è½¬æ¢ä¸æ‰§è¡Œã€‚"""
    map_list = {
        "col_decimal(40, 20)_undef_signed": "col_decimal_40_20_undef_signed",
        "col_decimal(40, 20)_undef_unsigned": "col_decimal_40_20_undef_unsigned",
        "col_decimal(40, 20)_key_signed": "col_decimal_40_20_key_signed",
        "col_decimal(40, 20)_key_unsigned": "col_decimal_40_20_key_unsigned",
        "col_char(20)_undef_signed": "col_char_20_undef_signed",
        "col_char(20)_key_signed": "col_char_20_key_signed",
        "col_varchar(20)_undef_signed": "col_varchar_20_undef_signed",
        "col_varchar(20)_key_signed": "col_varchar_20_key_signed",
    }
    sql_statement = origin_sql
    for key, value in map_list.items():
        if key in origin_sql:
            sql_statement = sql_statement.replace(key, value)
    return sql_statement


def get_examples_string(FewShot, origin_db, target_db):
    examples_string = ""
    examples_data = None
    if FewShot:  # ç»™å‡ºæ ·ä¾‹
        # exampleæ˜¯ç»™å‡ºçš„ä¾‹å­
        with open(
            os.path.join(
                "../../Dataset/Pinolo Output/output_test/",
                "examples_" + origin_db + "_" + target_db + ".json",
            ),
            "r",
            encoding="utf-8",
        ) as r:
            examples_data = json.load(r)
        example = None
        for index in range(len(examples_data)):
            example = examples_data[index]
            OriginDB = example["origin_db"]
            TargetDB = example["target_db"]
            SQL = example["Sql"]
            Answer = json.dumps(example["answer"])
            # åäº†ï¼Œè¿™å¥å¥½åƒå†™åœ¨forå¾ªç¯å¤–é¢äº†
            examples_string = (
                examples_string
                + "[Example "
                + str(index)
                + " start]: "
                + "[original database]: "
                + OriginDB
                + "\n"
                + "[target database]: "
                + TargetDB
                + "\n"
                + "[sql statement]: "
                + SQL
                + "\n"
                + "[Answer]: "
                + Answer
                + "\n"
                + "[Example "
                + str(index)
                + " end]\n"
            )
    return examples_string


def build_sql_to_redis_semantic_hints(sql_text):
    """
    æ ¹æ®SQLå†…å®¹åŒ¹é…sql_to_redis_mapping.jsonä¸­çš„patternï¼Œç”Ÿæˆç»“æ„åŒ–è¯­ä¹‰æç¤ºã€‚
    """
    import re

    mapping_path = os.path.join(
        "..",
        "..",
        "NoSQLFeatureKnowledgeBase",
        "Redis",
        "outputs",
        "sql_to_redis_mapping.json",
    )
    if not os.path.exists(mapping_path):
        return "[SQLâ†’Redis Semantic Mapping] (mapping file not found)\n"
    try:
        with open(mapping_path, "r", encoding="utf-8") as f:
            mapping_data = json.load(f)
        mappings = mapping_data.get("mappings", {})
    except Exception as e:
        return f"[SQLâ†’Redis Semantic Mapping Load Error]: {e}\n"

    sql = sql_text.lower()
    # patternä¼˜å…ˆçº§é¡ºåºï¼Œè¶Šå‰è¶Šä¼˜å…ˆ
    pattern_keys = [
        ("join", ["join"]),
        ("group by", ["group by"]),
        ("order by limit", ["order by", "limit"]),
        ("order by", ["order by"]),
        ("distinct", ["distinct"]),
        ("count(*)", ["count(*)", "count ( * )"]),
        ("update", ["update"]),
        ("delete", ["delete"]),
        ("select+where", ["select", "where"]),
        ("select by key", ["select"]),
    ]
    matched = []
    for key, keywords in pattern_keys:
        if all(kw in sql for kw in keywords):
            if key in mappings:
                matched.append((key, mappings[key]))
    # åªä¿ç•™æœ€å¤š3æ¡
    matched = matched[:3]
    if not matched:
        return "[SQLâ†’Redis Semantic Mapping] No typical pattern matched.\n"
    out = "[SQLâ†’Redis Semantic Mapping]\n"
    for key, val in matched:
        out += f"Pattern: {key}\n"
        if "redis" in val:
            out += f"Redis Strategy: {val['redis']}\n"
        if "example" in val:
            out += f"Example: {val['example']}\n"
        if "notes" in val:
            out += f"Notes: {val['notes']}\n"
        if "tradeoffs" in val:
            out += f"Tradeoffs: {val['tradeoffs']}\n"
        if "pitfalls" in val:
            out += f"Pitfalls: {val['pitfalls']}\n"
        out += "\n"
    return out


"""
def get_feature_knowledge_string(origin_db, target_db, with_knowledge, mapping_indexes):
    knowledge_string = ""
    steps_string = ""
    examples_data = None
    if with_knowledge:  # ç»™å‡ºæ ·ä¾‹
        # è·å–å¯¹åº”çš„è¯¦ç»†ä¿¡æ¯
        names_ = "merge"
        for feature_type in ["function"]:
            names_ = names_ + "_" + feature_type
        origin_merge_feature_filename = os.path.join("..", "..", "FeatureKnowledgeBase Processed1", origin_db, "RAG_Embedding_Data",names_ + ".jsonl")
        target_merge_feature_filename = os.path.join("..", "..", "FeatureKnowledgeBase Processed1", target_db,"RAG_Embedding_Data", names_ + ".jsonl")
        with open(origin_merge_feature_filename, "r", encoding="utf-8") as r:
            origin_features = r.readlines()
        with open(target_merge_feature_filename, "r", encoding="utf-8") as r:
            target_features = r.readlines()
        cnt = 0
        for mapping_pair in mapping_indexes:
            # è·å–a_dbä»¥åŠb_dbä¸­çš„feature knowledge
            origin_feature = json.loads(origin_features[mapping_pair[0]])
            target_feature = json.loads(target_features[mapping_pair[1]])
            knowledge_string = knowledge_string + "[Feature Mapping "+str(cnt) + " Start]" + "Here is the original feature.\n"
            knowledge_string = knowledge_string + "Feature Syntax(Database "+origin_db+"):"
            for item in origin_feature["Feature"]:
                knowledge_string += item
            knowledge_string += "\nDescription(Database "+origin_db+"):"
            for item in origin_feature["Description"]:
                knowledge_string += item
            knowledge_string = knowledge_string + "\nExamples(Database "+origin_db+"):"
            for item in origin_feature["Examples"]:
                knowledge_string += item

            knowledge_string += "Here is the mapping feature.\n"

            knowledge_string = knowledge_string + "Feature Syntax(Database "+target_db+"):"
            for item in target_feature["Feature"]:
                knowledge_string += item
            knowledge_string = knowledge_string + "\nDescription(Database "+origin_db+"):"
            for item in target_feature["Description"]:
                knowledge_string += item
            knowledge_string = knowledge_string + "\nExamples(Database "+origin_db+"):"
            for item in target_feature["Examples"]:
                knowledge_string += item
            knowledge_string += "\n"

            knowledge_string = knowledge_string + "[Feature Mapping " + str(cnt) + " End]"

            # æ‹¼æ¥stepsçš„æç¤ºè¯
            steps_string = steps_string + "Replace "+str(origin_feature["Feature"])+" from "+origin_db+" in original sql with "+str(target_feature["Feature"])+" from "+target_db+"\n"
            cnt += 1
    return knowledge_string, steps_string
"""

"""
def get_NoSQL_knowledge_string(
    origin_db, target_db, with_knowledge, sql_statement_processed
):
    # åˆå§‹åŒ–è¿”å›å­—ç¬¦ä¸²ï¼Œé¿å…åœ¨å¼‚å¸¸æˆ–é Redis åˆ†æ”¯æ—¶æœªå®šä¹‰
    knowledge_string = ""
    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ¥æºæˆ–ç›®æ ‡æ˜¯ Redisï¼Œåˆ™åŠ è½½ NoSQL Redis çŸ¥è¯†åº“
    # ç›®å‰å®ç°ï¼šå½“ origin_db ä¸º redis æ—¶ï¼Œæ³¨å…¥ Redis å‘½ä»¤/ç¤ºä¾‹çŸ¥è¯†ï¼›
    # åç»­å¯æ‰©å±• target_db == redis çš„æ˜ å°„æç¤ºï¼ˆæ¯”å¦‚ SQL -> Redisï¼‰ã€‚
    if str(origin_db).lower() == "redis":
        try:
            repo_root = os.path.dirname(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            )
            redis_kb_path = os.path.join(
                repo_root,
                "NoSQLFeatureKnowledgeBase",
                "Redis",
                "outputs",
                "redis_commands_knowledge.json",
            )
            print("Loading Redis CMD Knowledge from: " + redis_kb_path)

            if os.path.exists(redis_kb_path):
                with open(redis_kb_path, "r", encoding="utf-8") as rf:
                    redis_kb = json.load(rf)
                commands = redis_kb.get("commands", {})
                # æ ¹æ® sql_statement_processed ä¸­å‡ºç°çš„å‘½ä»¤å…³é”®è¯åŠ¨æ€æŠ½å–
                import re

                statement_text = (sql_statement_processed or "").lower()

                # æå–æ‰€æœ‰å­—æ¯/æ•°å­—/_/: ç»„æˆçš„ token ä½œä¸ºæ½œåœ¨å‘½ä»¤ï¼ˆå•è¯å½¢å¼ï¼‰
                raw_tokens = set(re.findall(r"[a-zA-Z][a-zA-Z0-9:_]*", statement_text))

                matched = []
                for cmd_name in commands.keys():
                    cmd_lower = cmd_name.lower()
                    # å¤šè¯å‘½ä»¤ï¼ˆå¦‚ CLIENT LISTï¼‰é‡‡ç”¨æ•´ä½“æ­£åˆ™åŒ¹é…ï¼›å•è¯å‘½ä»¤ç›´æ¥åœ¨ token é›†åˆé‡Œåˆ¤æ–­
                    if " " in cmd_lower:
                        if re.search(
                            r"\b" + re.escape(cmd_lower) + r"\b", statement_text
                        ):
                            matched.append(cmd_name)
                    else:
                        if cmd_lower in raw_tokens:
                            matched.append(cmd_name)

                # å»é‡ä¿æŒå‡ºç°é¡ºåºï¼šæŒ‰åœ¨æ–‡æœ¬ä¸­é¦–æ¬¡å‡ºç°çš„ä½ç½®æ’åº
                def first_pos(name: str):
                    try:
                        return statement_text.index(name.lower())
                    except ValueError:
                        return 10**9

                matched = sorted(list(dict.fromkeys(matched)), key=first_pos)

                if not matched:
                    knowledge_string += "(no command keyword matched in input statement; showing fallback examples)\n"
                    # å›é€€æŒ‘é€‰å¸¸è§å‘½ä»¤ï¼Œé¿å… prompt ä¸ºç©º
                    fallback_order = [
                        "GET",
                        "SET",
                        "DEL",
                        "HSET",
                        "HGET",
                        "LPUSH",
                        "RPUSH",
                        "SADD",
                        "ZADD",
                        "EVAL",
                    ]
                    matched = [c for c in fallback_order if c in commands][:5]

                for cmd_name in matched:
                    meta = commands.get(cmd_name, {})
                    examples = meta.get("examples", [])
                    example_snippet = ""
                    if examples:
                        # é€‰å–æœ€çŸ­ç¤ºä¾‹ï¼Œé¿å…è¿‡é•¿
                        shortest = min(examples, key=lambda x: len(x.get("raw", "")))
                        example_snippet = shortest.get("raw", "")
                    knowledge_string += f"Command: {cmd_name}\n"
                    if meta.get("group"):
                        knowledge_string += f"Group: {meta.get('group')}\n"
                    if meta.get("summary"):
                        knowledge_string += f"Summary: {meta.get('summary')}\n"
                    if meta.get("since"):
                        knowledge_string += f"Since: {meta.get('since')}\n"
                    if meta.get("complexity"):
                        knowledge_string += f"Complexity: {meta.get('complexity')}\n"
                    if example_snippet:
                        knowledge_string += f"Example: {example_snippet}\n"
                    knowledge_string += "\n"

                # é¿å… LLM è‡†é€ ä»»æ„è¡¨å
                # ä»…å½“ç›®æ ‡æ˜¯å…¸å‹å…³ç³»å‹æ•°æ®åº“ï¼ˆå¦‚ postgres / mysql / mariadb / tidb / sqlite / duckdb / clickhouse / monetdb / postgres ç­‰ï¼‰æ—¶æ³¨å…¥
                relational_targets = {
                    "postgres",
                    "mysql",
                    "mariadb",
                    "tidb",
                    "sqlite",
                    "duckdb",
                    "clickhouse",
                    "monetdb",
                }
                nosql_targets = {"redis", "mongodb"}
                if str(target_db).lower() in relational_targets:
                    knowledge_string += "You MUST NOT invent other table names. Always reuse tab_0 tab_1.\n"
                elif str(target_db).lower() in nosql_targets:
                    # é’ˆå¯¹ MongoDB ç›®æ ‡æ·»åŠ ä¸¥æ ¼ JSON è¾“å‡ºçº¦æŸï¼ˆç¦æ­¢ä½¿ç”¨ shell é£æ ¼ "db.collection.find()" å½¢å¼ï¼‰
                    if str(target_db).lower() == "mongodb":
                        transfer_llm_string = ""
            else:
                knowledge_string += "Knowl file not found\n"
        except Exception as e:
            knowledge_string = ""
        # Redis å½“å‰ä¸ä½¿ç”¨ mapping_indexesï¼ˆå‘½ä»¤ä¹‹é—´æš‚æœªå»ºç«‹æ˜ å°„å¯¹ï¼‰ï¼Œç›´æ¥è¿”å›
        print("knowledge_string: " + knowledge_string)
        return knowledge_string
    # é Redis æƒ…å†µè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆè°ƒç”¨å¤„ä¼šæ ¹æ® with_knowledge é€»è¾‘å†³å®šåç»­ï¼‰
    return knowledge_string


"""


def get_NoSQL_knowledge_string(
    origin_db, target_db, with_knowledge, sql_statement_processed
):
    relational_targets = {
        "postgres",
        "mysql",
        "mariadb",
        "tidb",
        "sqlite",
        "duckdb",
        "clickhouse",
        "monetdb",
    }
    nosql_targets = {"redis", "mongodb", "surrealdb"}
    # åˆå§‹åŒ–è¿”å›å­—ç¬¦ä¸²ï¼Œé¿å…åœ¨å¼‚å¸¸æˆ–é Redis åˆ†æ”¯æ—¶æœªå®šä¹‰
    knowledge_string = ""
    
    # === SurrealDB çŸ¥è¯†åº“åŠ è½½ ===
    if str(target_db).lower() == "surrealdb":
        try:
            repo_root = os.path.dirname(
                os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            )
            
            # åŠ è½½ SQL åˆ° SurrealDB æ˜ å°„
            mapping_path = os.path.join(
                repo_root,
                "NoSQLFeatureKnowledgeBase",
                "SurrealDB",
                "sql_to_surrealdb_mapping.json"
            )
            
            if os.path.exists(mapping_path):
                with open(mapping_path, "r", encoding="utf-8") as f:
                    surrealdb_kb = json.load(f)
                
                knowledge_string += "\n========== ğŸ”´ CRITICAL SurrealDB Syntax Rules ==========\n\n"
                
                # 1. æœ€å…³é”®ï¼šCREATE TABLE è¯­æ³•
                if "critical_syntax_differences" in surrealdb_kb:
                    create_table_info = surrealdb_kb["critical_syntax_differences"].get("CREATE_TABLE", {})
                    knowledge_string += "âš ï¸  CREATE TABLE Syntax (MOST IMPORTANT!):\n\n"
                    knowledge_string += f"SQLite pattern: {create_table_info.get('sqlite_pattern', 'N/A')}\n"
                    knowledge_string += f"SurrealDB pattern: {create_table_info.get('surrealdb_pattern', 'N/A')}\n\n"
                    knowledge_string += "âŒ WRONG Examples (DO NOT USE):\n"
                    for err in create_table_info.get("common_errors", [])[:3]:
                        knowledge_string += f"  - {err}\n"
                    knowledge_string += "\nâœ… CORRECT Examples:\n"
                    for trans in create_table_info.get("correct_translations", [])[:3]:
                        knowledge_string += f"  Input:  {trans['input']}\n"
                        knowledge_string += f"  Output: {trans['output']}\n\n"
                    knowledge_string += f"Note: {create_table_info.get('notes', '')}\n\n"
                
                # 2. ç±»å‹æ˜ å°„
                if "type_mappings" in surrealdb_kb:
                    knowledge_string += "Type Mappings:\n"
                    for sql_type, surreal_type in surrealdb_kb["type_mappings"].items():
                        knowledge_string += f"  {sql_type} â†’ {surreal_type}\n"
                    knowledge_string += "\n"
                
                # 3. èšåˆå‡½æ•°
                if "aggregate_functions" in surrealdb_kb:
                    knowledge_string += "Aggregate Functions:\n"
                    for func_name, func_info in surrealdb_kb["aggregate_functions"].items():
                        knowledge_string += f"  {func_name} â†’ {func_info.get('surrealdb', 'N/A')}"
                        if func_info.get("notes"):
                            knowledge_string += f" ({func_info['notes']})"
                        knowledge_string += "\n"
                    knowledge_string += "\n"
                
                # 4. ä¸æ”¯æŒçš„ç‰¹æ€§
                if "unsupported_features" in surrealdb_kb:
                    knowledge_string += "âš ï¸  Unsupported Features (Return comment):\n"
                    for feature_name in surrealdb_kb["unsupported_features"].keys():
                        knowledge_string += f"  - {feature_name}\n"
                    knowledge_string += "\n"
                
                knowledge_string += "========================================\n\n"
                print(f"âœ… Loaded SurrealDB knowledge base from: {mapping_path}")
            else:
                print(f"âš ï¸  SurrealDB knowledge base not found: {mapping_path}")
        
        except Exception as e:
            print(f"âŒ Failed to load SurrealDB knowledge base: {e}")
    
    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ¥æºæˆ–ç›®æ ‡æ˜¯ Redisï¼Œåˆ™åŠ è½½ NoSQL Redis çŸ¥è¯†åº“
    # ç›®å‰å®ç°ï¼šå½“ origin_db ä¸º redis æ—¶ï¼Œæ³¨å…¥ Redis å‘½ä»¤/ç¤ºä¾‹çŸ¥è¯†ï¼›
    # åç»­å¯æ‰©å±• target_db == redis çš„æ˜ å°„æç¤ºï¼ˆæ¯”å¦‚ SQL -> Redisï¼‰ã€‚
    elif str(origin_db).lower() == "redis":
        try:
            # repo_root = os.path.dirname(
            #     os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            # )
            # redis_kb_path = os.path.join(
            #     repo_root,
            #     "NoSQLFeatureKnowledgeBase",
            #     "Redis",
            #     "outputs",
            #     "redis_commands_knowledge.json",
            # )
            # print("Loading Redis CMD Knowledge from: " + redis_kb_path)
            # é¿å… LLM è‡†é€ ä»»æ„è¡¨å
            # ä»…å½“ç›®æ ‡æ˜¯å…¸å‹å…³ç³»å‹æ•°æ®åº“ï¼ˆå¦‚ postgres / mysql / mariadb / tidb / sqlite / duckdb / clickhouse / monetdb / postgres ç­‰ï¼‰æ—¶æ³¨å…¥
            if str(target_db).lower() in relational_targets:
                knowledge_string += (
                    "You MUST NOT invent other table names. Always reuse tab_0 tab_1.\n"
                )
            elif str(target_db).lower() in nosql_targets:
                # é’ˆå¯¹ MongoDB ç›®æ ‡å¼ºåˆ¶ä½¿ç”¨ Shell è¯­æ³•
                if str(target_db).lower() == "mongodb":
                    knowledge_string += """Additional Target-Specific Output Constraints (MongoDB):
                    You MUST output the field TransferSQL as MongoDB Shell command that can be executed directly in mongosh.
                    
                    Output Format Requirements:
                    - Use MongoDB Shell syntax: db.<collection>.<method>(...)
                    - End each command with a semicolon (;)
                    - Use proper JavaScript/MongoDB syntax
                    - Support all MongoDB operations: insertOne, findOne, find, updateOne, deleteOne, aggregate
                    - Use MongoDB operators: $set, $inc, $exists, $gt, $lt, $eq, $type, etc.
                    - Include options like upsert true, .sort(), .limit(), .skip() when needed
                    
                    Translation Patterns for Redis to MongoDB:
                    
                    1. Key-Value Operations:
                       - SET mykey hello â†’ db.myCollection.insertOne({{ _id: "mykey", value: "hello" }});
                       - GET mykey â†’ db.myCollection.findOne({{ _id: "mykey" }});
                       - DEL mykey â†’ db.myCollection.deleteOne({{ _id: "mykey" }});
                       - EXISTS mykey â†’ db.myCollection.findOne({{ _id: "mykey" }});
                    
                    2. Counter Operations:
                       - SET counter 1 â†’ db.myCollection.insertOne({{ _id: "counter", value: 1 }});
                       - INCR counter â†’ db.myCollection.updateOne({{ _id: "counter" }}, {{ $inc: {{ value: 1 }} }}, {{ upsert: true }});
                       - DECR counter â†’ db.myCollection.updateOne({{ _id: "counter" }}, {{ $inc: {{ value: -1 }} }}, {{ upsert: true }});
                       - GET counter â†’ db.myCollection.findOne({{ _id: "counter" }});
                    
                    3. Sorted Set Operations:
                       - ZADD myset 100 member1 â†’ db.zset.insertOne({{ key: "myset", member: "member1", score: 100 }});
                       - ZRANGE myset 0 10 â†’ db.zset.find({{ key: "myset" }}).sort({{ score: 1 }}).skip(0).limit(11);
                       - ZCOUNT myset 0 100 â†’ db.zset.countDocuments({{ key: "myset", score: {{ $gte: 0, $lte: 100 }} }});
                    
                    Critical Rules:
                    - MUST use db.<collection>.<method>() format
                    - MUST end with semicolon (;)
                    - Use actual field/key names from the original command
                    - For key-value storage, use collection name "myCollection" and field "_id" as key
                    - For sorted sets, use collection name "zset"
                    - NO markdown code fences, NO explanatory text in TransferSQL field
                    - TransferSQL should contain ONLY the executable MongoDB Shell command
                    
                    Example Output:
                    Use double curly braces for MongoDB JSON objects in your response.
                    TransferSQL should be like: db.myCollection.findOne({{ _id: "mykey" }});
                    Explanation should describe the conversion logic.
                    """
        except Exception as e:
            knowledge_string = ""
        # Redis å½“å‰ä¸ä½¿ç”¨ mapping_indexesï¼ˆå‘½ä»¤ä¹‹é—´æš‚æœªå»ºç«‹æ˜ å°„å¯¹ï¼‰ï¼Œç›´æ¥è¿”å›
        print("knowledge_string: " + knowledge_string)
        return knowledge_string
    # é Redis æƒ…å†µè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆè°ƒç”¨å¤„ä¼šæ ¹æ® with_knowledge é€»è¾‘å†³å®šåç»­ï¼‰
    return knowledge_string


# å‡½æ•°å®šä¹‰ï¼šè·å–ç‰¹å¾çŸ¥è¯†å­—ç¬¦ä¸²ï¼Œç”¨äºæ„å»ºä»æºæ•°æ®åº“åˆ°ç›®æ ‡æ•°æ®åº“çš„ç‰¹å¾æ˜ å°„çŸ¥è¯†
def get_feature_knowledge_string(
    origin_db, target_db, with_knowledge, mapping_indexes, sql_statement_processed
):
    # åˆå§‹åŒ–çŸ¥è¯†å­—ç¬¦ä¸²ä¸ºç©ºï¼Œç”¨äºç´¯ç§¯ç‰¹å¾çŸ¥è¯†æè¿°
    knowledge_string = ""
    # åˆå§‹åŒ–æ­¥éª¤å­—ç¬¦ä¸²ä¸ºç©ºï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰
    steps_string = ""
    # åˆå§‹åŒ–ç¤ºä¾‹æ•°æ®ä¸ºNoneï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰
    examples_data = None
    # å¦‚æœéœ€è¦åŒ…å«çŸ¥è¯†ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹é€»è¾‘
    if with_knowledge:  # ç»™å‡ºæ ·ä¾‹
        # è·å–å¯¹åº”çš„è¯¦ç»†ä¿¡æ¯
        # åˆå§‹åŒ–æ–‡ä»¶ååŸºç¡€éƒ¨åˆ†
        names_ = "merge"
        # ä¸ºæ¯ä¸ªç‰¹å¾ç±»å‹æ·»åŠ åç¼€ï¼Œè¿™é‡Œåªå¤„ç†"function"
        for feature_type in ["function"]:
            names_ = names_ + "_" + feature_type
        # æ„å»ºæºæ•°æ®åº“çš„åˆå¹¶ç‰¹å¾æ–‡ä»¶åè·¯å¾„
        origin_merge_feature_filename = os.path.join(
            "..",
            "..",
            "FeatureKnowledgeBase",
            origin_db,
            "RAG_Embedding_Data",
            names_ + ".jsonl",
        )
        print("origin_merge_feature_filename: " + origin_merge_feature_filename)
        # æ„å»ºç›®æ ‡æ•°æ®åº“çš„åˆå¹¶ç‰¹å¾æ–‡ä»¶åè·¯å¾„
        target_merge_feature_filename = os.path.join(
            "..",
            "..",
            "FeatureKnowledgeBase",
            target_db,
            "RAG_Embedding_Data",
            names_ + ".jsonl",
        )
        print("target_merge_feature_filename: " + target_merge_feature_filename)
        # ä»¥åªè¯»æ¨¡å¼æ‰“å¼€æºç‰¹å¾æ–‡ä»¶ï¼Œå¹¶è¯»å–æ‰€æœ‰è¡Œ
        with open(origin_merge_feature_filename, "r", encoding="utf-8") as r:
            origin_features = r.readlines()
        # ä»¥åªè¯»æ¨¡å¼æ‰“å¼€ç›®æ ‡ç‰¹å¾æ–‡ä»¶ï¼Œå¹¶è¯»å–æ‰€æœ‰è¡Œ
        with open(target_merge_feature_filename, "r", encoding="utf-8") as r:
            target_features = r.readlines()
        # åˆå§‹åŒ–è®¡æ•°å™¨ï¼Œç”¨äºæ­¥éª¤ç¼–å·
        cnt = 0
        # éå†æ˜ å°„ç´¢å¼•å¯¹ï¼Œæ¯ä¸ªå¯¹åŒ…å«æºå’Œç›®æ ‡ç‰¹å¾çš„ç´¢å¼•
        for mapping_pair in mapping_indexes:
            # è·å–a_dbä»¥åŠb_dbä¸­çš„feature knowledge
            # ä»æºç‰¹å¾åˆ—è¡¨ä¸­è§£æå¯¹åº”ç´¢å¼•çš„ç‰¹å¾JSON
            origin_feature = json.loads(origin_features[mapping_pair[0]])
            # ä»ç›®æ ‡ç‰¹å¾åˆ—è¡¨ä¸­è§£æå¯¹åº”ç´¢å¼•çš„ç‰¹å¾JSON
            target_feature = json.loads(target_features[mapping_pair[1]])
            # æ„å»ºæ­¥éª¤æè¿°å­—ç¬¦ä¸²ï¼Œè¯´æ˜ä»æºåˆ°ç›®æ ‡çš„è½¬æ¢
            knowledge_string = (
                knowledge_string
                + " Step "
                + str(cnt)
                + ": Transfer "
                + str(origin_feature["Feature"])
                + " from "
                + origin_db
                + " to "
                + str(target_feature["Feature"])
                + " from "
                + target_db
                + "\n"
            )
            # æ·»åŠ æºç‰¹å¾çš„ä»‹ç»æ–‡æœ¬
            knowledge_string = (
                knowledge_string
                + "Here is the original feature from "
                + origin_db
                + ".\n"
            )
            # æ·»åŠ æºç‰¹å¾çš„è¯­æ³•æè¿°å‰ç¼€
            knowledge_string = (
                knowledge_string + "Feature Syntax(Database " + origin_db + "):"
            )
            # éå†æºç‰¹å¾çš„è¯­æ³•é¡¹å¹¶è¿½åŠ åˆ°å­—ç¬¦ä¸²
            for item in origin_feature["Feature"]:
                knowledge_string += item
            # æ³¨é‡Šæ‰çš„ä»£ç ï¼šæ·»åŠ æºç‰¹å¾çš„æè¿°ï¼ˆå½“å‰è¢«æ³¨é‡Šï¼‰
            """
            knowledge_string += "\nDescription(Database "+origin_db+"):"
            for item in origin_feature["Description"]:
                knowledge_string += item
            """
            # æ·»åŠ æºç‰¹å¾çš„ç¤ºä¾‹å‰ç¼€
            knowledge_string = (
                knowledge_string + "\nExamples(Database " + origin_db + "):"
            )
            # éå†æºç‰¹å¾çš„ç¤ºä¾‹é¡¹å¹¶è¿½åŠ åˆ°å­—ç¬¦ä¸²
            for item in origin_feature["Examples"]:
                knowledge_string += item
            # æ·»åŠ ç›®æ ‡ç‰¹å¾çš„ä»‹ç»æ–‡æœ¬
            knowledge_string = (
                knowledge_string
                + "Here is the mapping feature from "
                + target_db
                + ".\n"
            )
            # æ·»åŠ ç›®æ ‡ç‰¹å¾çš„è¯­æ³•æè¿°å‰ç¼€
            knowledge_string = (
                knowledge_string + "Feature Syntax(Database " + target_db + "):"
            )
            # éå†ç›®æ ‡ç‰¹å¾çš„è¯­æ³•é¡¹å¹¶è¿½åŠ åˆ°å­—ç¬¦ä¸²
            for item in target_feature["Feature"]:
                knowledge_string += item
            # æ³¨é‡Šæ‰çš„ä»£ç ï¼šæ·»åŠ ç›®æ ‡ç‰¹å¾çš„æè¿°ï¼ˆå½“å‰è¢«æ³¨é‡Šï¼‰
            """
            knowledge_string = knowledge_string + "\nDescription(Database "+origin_db+"):"
            for item in target_feature["Description"]:
                knowledge_string += item
            """
            # æ·»åŠ ç›®æ ‡ç‰¹å¾çš„ç¤ºä¾‹å‰ç¼€
            knowledge_string = (
                knowledge_string + "\nExamples(Database " + origin_db + "):"
            )
            # éå†ç›®æ ‡ç‰¹å¾çš„ç¤ºä¾‹é¡¹å¹¶è¿½åŠ åˆ°å­—ç¬¦ä¸²
            for item in target_feature["Examples"]:
                knowledge_string += item
            # æ·»åŠ ä¸¤ä¸ªæ¢è¡Œç¬¦ä»¥åˆ†éš”ä¸åŒæ˜ å°„
            knowledge_string += "\n\n"
            # è®¡æ•°å™¨é€’å¢
            cnt += 1
    # è¿”å›æ„å»ºçš„çŸ¥è¯†å­—ç¬¦ä¸²
    return knowledge_string


# ============ Transfer Agent (ç”¨äº Redisâ†’MongoDB ç­‰ NoSQL è½¬æ¢) ============


def _build_transfer_agent(origin_db: str, target_db: str) -> Optional[Any]:
    """åˆ›å»º Transfer Agent ç”¨äºè·¨æ•°æ®åº“è¯­å¥è½¬æ¢ã€‚

    ä¼˜åŠ¿:
    - å¯è°ƒç”¨å·¥å…·æŸ¥è¯¢ NoSQL è§„åˆ™å’Œç¤ºä¾‹
    - ä¸¥æ ¼æ§åˆ¶ JSON æ ¼å¼è¾“å‡º
    - é€‚ç”¨äº Redisâ†’MongoDB ç­‰ NoSQL åœºæ™¯
    """
    try:
        from langchain_openai import ChatOpenAI
        from langchain.agents import AgentExecutor, create_openai_functions_agent
        from langchain.tools import tool
        from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
    except Exception as e:
        print(f"âŒ Failed to import LangChain dependencies for Transfer Agent: {e}")
        print(f"   Error type: {type(e).__name__}")
        print(f"   This is likely due to LangChain version incompatibility.")
        print(f"   Falling back to traditional LLM mode...")
        return None

    # å®šä¹‰å·¥å…·
    @tool
    def get_mongodb_shell_examples() -> str:
        """Returns MongoDB shell command examples for common Redis operations."""
        return """MongoDB Shell Command Examples (can be executed in mongosh):

**CRITICAL: Use actual values from the Redis command, NOT placeholders!**

Redis â†’ MongoDB Conversion Patterns (with REAL values):
- SET mykey hello â†’ db.myCollection.insertOne({ _id: "mykey", value: "hello" })
- GET mykey â†’ db.myCollection.findOne({ _id: "mykey" })
- SET counter 42 â†’ db.myCollection.insertOne({ _id: "counter", value: 42 }, { upsert: true })
- INCR counter â†’ db.myCollection.updateOne({ _id: "counter" }, { $inc: { value: 1 } }, { upsert: true })
- DEL mykey â†’ db.myCollection.deleteOne({ _id: "mykey" })
- EXISTS mykey â†’ db.myCollection.findOne({ _id: "mykey" })
- ZADD myset 100 member1 â†’ db.zset.insertOne({ key: "myset", member: "member1", score: 100 })
- ZRANGE myset 0 10 â†’ db.zset.find({ key: "myset" }).sort({ score: 1 }).skip(0).limit(11)

Key Points:
- **MUST use actual key/value names from the Redis command**
- Use db.<collection>.<method>(...) format
- Common methods: insertOne, findOne, find, updateOne, deleteOne
- Operators: $set, $inc, $exists, $gt, $lt, etc.
- Options: { upsert: true }, .sort(), .limit(), .skip()
"""

    @tool
    def validate_mongodb_syntax(cmd: str) -> str:
        """Basic validation for MongoDB shell command syntax."""
        if not cmd.strip().startswith("db."):
            return "invalid: must start with 'db.'"
        if not any(
            op in cmd
            for op in [
                "insertOne",
                "findOne",
                "find",
                "updateOne",
                "deleteOne",
                "aggregate",
            ]
        ):
            return "warn: missing common MongoDB operation"
        return "valid"

    tools = [get_mongodb_shell_examples, validate_mongodb_syntax]

    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                f"You are a database translation expert specializing in {origin_db} to {target_db} conversion. "
                "Your task is to convert database commands while maintaining semantic equivalence.\n\n"
                "**OUTPUT FORMAT for MongoDB target:**\n"
                "- Return MongoDB shell commands that can be executed directly in mongosh\n"
                "- Use db.<collection>.<method>(...) format (e.g., db.myCollection.findOne(...))\n"
                "- Support all MongoDB operations: insertOne, findOne, find, updateOne, deleteOne, aggregate\n"
                "- Use MongoDB operators: $set, $inc, $exists, $gt, $type, etc.\n"
                "- Include options like {{ upsert: true }}, .sort(), .limit() when needed\n"
                "- Maintain original field names from the source command\n"
                "- End each command with a semicolon\n\n"
                "**RESPONSE FORMAT:**\n"
                '- Return JSON: {{"TransferSQL": "<shell_command>", "Explanation": "<text>"}}\n'
                "- TransferSQL should be a valid MongoDB shell command\n"
                "- Example: db.myCollection.findOne({{ counter: {{ $exists: true }} }});\n\n"
                "If you need examples, call get_mongodb_shell_examples tool first.",
            ),
            ("user", "{{input}}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ]
    )

    llm = ChatOpenAI(
        model=os.environ.get("OPENAI_TRANSFER_AGENT_MODEL", "gpt-4o-mini"),
        temperature=0.3,
    )
    agent = create_openai_functions_agent(llm, tools, prompt)
    return AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=False,
        max_iterations=10,
        return_intermediate_steps=False,
        early_stopping_method="generate",
    )


def _agent_transfer_statement(
    origin_db: str, target_db: str, sql_statement: str
) -> Optional[Dict[str, str]]:
    """ä½¿ç”¨ Agent è¿›è¡Œè¯­å¥è½¬æ¢ï¼Œè¿”å› {"TransferSQL": ..., "Explanation": ...} æˆ– Noneã€‚"""
    agent = _build_transfer_agent(origin_db, target_db)
    if agent is None:
        print("Failed to build transfer agent.")
        return None

    input_text = (
        f"Convert the following {origin_db} command to {target_db} shell format:\n"
        f"{sql_statement}\n\n"
        f"Requirements:\n"
        f"1. Use actual values from the command (e.g., 'set mykey hello' â†’ _id: 'mykey', value: 'hello')\n"
        f"2. For MongoDB: return shell command like db.collection.method(...) ending with semicolon\n"
        f"3. Maintain semantic equivalence\n"
        f'4. Return JSON: {{"TransferSQL": "<shell_command>;", "Explanation": "<text>"}}\n'
        f"5. Example: Redis 'set foo bar' â†’ MongoDB 'db.myCollection.insertOne({{ _id: \"foo\", value: \"bar\" }});'"
    )
    print("ğŸ“¥ Agent Input: " + input_text)
    try:
        res = agent.invoke({"input": input_text})
        print(f"ğŸ“¥ âœ… Agent è°ƒç”¨æˆåŠŸï¼Œè¿”å›ç±»å‹: {type(res)}")
        output = res.get("output") if isinstance(res, dict) else None
        if not output:
            print(f"ğŸ“¥ âŒ Agent è¿”å›ç»“æœä¸­æ²¡æœ‰ 'output' å­—æ®µ: {res}")
            return None

        print(f"ğŸ“¥ ğŸ“¤ Agent åŸå§‹è¾“å‡º: {output}")

        # æå– JSON
        txt = str(output).strip()

        # ç§»é™¤ä»£ç å—æ ‡è®°
        if "```json" in txt:
            start = txt.find("```json") + 7
            end = txt.find("```", start)
            if end > start:
                txt = txt[start:end].strip()
        elif "```" in txt:
            start = txt.find("```") + 3
            end = txt.find("```", start)
            if end > start:
                txt = txt[start:end].strip()

        # æå– JSON å¯¹è±¡
        if not txt.startswith("{"):
            first_brace = txt.find("{")
            last_brace = txt.rfind("}")
            if first_brace >= 0 and last_brace > first_brace:
                txt = txt[first_brace : last_brace + 1]

        print(f"ğŸ“¥ ğŸ”§ æå–çš„ JSON æ–‡æœ¬: {txt}")

        # ä¿®å¤è½¬ä¹‰
        txt = txt.replace("\\$", "\\\\$")

        data = json.loads(txt)
        print(f"ğŸ“¥ âœ… JSON è§£ææˆåŠŸ: {data}")
        return data
    except Exception as e:
        print(f"ğŸ“¥ âŒ Agent Transfer å¤±è´¥: {type(e).__name__}: {e}")
        import traceback

        traceback.print_exc()
        return None


def transfer_llm_sql_semantic(
    tool,
    exp,
    conversation,
    error_iteration,
    iteration_num,
    FewShot,
    with_knowledge,
    origin_db,
    target_db,
    test_info,
    use_redis_kb: bool = False,
):
    """
    # transfer llm:å•æ¡sqlè¯­å¥çš„è½¬æ¢åŠç»“æœå¤„ç†
    # è¿”å›ç»“æœï¼šcosts, transfer_results, exec_results, exec_times, error_messages, str(origin_exec_result), str(origin_exec_time), str(origin_error_message), exec_equalities
    :return:
    # * transfer llmçš„èŠ±é”€åˆ—è¡¨"costs",ç»“æœåˆ—è¡¨"transfer_results"ï¼Œ
    # * è½¬æ¢åè¯­å¥Sqlçš„è¿è¡Œç»“æœåˆ—è¡¨"exec_results"ï¼Œè¿è¡ŒæŠ¥é”™åˆ—è¡¨"error_messages"ï¼Œ
    # * è½¬æ¢å‰è¯­å¥Sqlï¼Œå¯¹åº”è¿è¡Œç»“æœ"str(origin_exec_result)"ï¼Œè¿è¡ŒæŠ¥é”™"str(origin_error_message)"
    # * è¿è¡Œç»“æœä¸åŸsqlçš„ä¸€è‡´æ€§åˆ—è¡¨"exec_equalities"
    # * åˆ—è¡¨æ˜¯ä¸ºè¿”å›errorè¿›è¡Œè¿­ä»£è®¾è®¡çš„ï¼Œèƒ½è®°å½•å¤šæ¬¡è¿­ä»£çš„è¿‡ç¨‹å€¼
    """
    # ========== Mem0 è®°å¿†ç®¡ç†åˆå§‹åŒ– ==========
    use_mem0 = os.environ.get("QTRAN_USE_MEM0", "false").lower() == "true"
    mem0_manager = None
    if use_mem0:
        try:
            from src.TransferLLM.mem0_adapter import TransferMemoryManager, FallbackMemoryManager
            try:
                mem0_manager = TransferMemoryManager(
                    user_id=f"qtran_{origin_db}_to_{target_db}"
                )
                print(f"âœ… Mem0 initialized for {origin_db} -> {target_db}")
            except ImportError:
                print("âš ï¸ Mem0 not available, using fallback memory manager")
                mem0_manager = FallbackMemoryManager(
                    user_id=f"qtran_{origin_db}_to_{target_db}"
                )
            
            # å¼€å¯ç¿»è¯‘ä¼šè¯
            molt = test_info.get("molt", "unknown")
            mem0_manager.start_session(
                origin_db=origin_db,
                target_db=target_db,
                molt=molt
            )
        except Exception as e:
            print(f"âš ï¸ Failed to initialize Mem0: {e}, continuing without memory")
            mem0_manager = None
    
    # test_info: {'index': 162, 'a_db': 'sqlite', 'b_db': 'duckdb', 'molt': 'norec', 'sqls': 'CREATE TABLE t0(c0);'}
    sql_statement = test_info["sql"]
    sql_statement_processed = sql_statement

    # åœ¨è¿™é‡Œåšsqlé¢„å¤„ç†
    # å¦‚æœæ˜¯mysql_likeè½¬postgresï¼Œå…ˆæŠŠsqlè¯­å¥çš„åˆ—åè¿›è¡Œæ›¿æ¢ï¼Œå’Œpostgresçš„ddlè¯­å¥ä¸­çš„åˆ—åä¿æŒä¸€è‡´ï¼Œä¾¿äºåç»­è¯­å¥è½¬æ¢
    if target_db == "postgres" and tool.lower() == "pinolo":
        sql_statement_processed = sql_statement_process(sql_statement)

    # è¯´æ˜ï¼štransfer_llm_string æ˜¯å‘é€ç»™ LLM çš„ä¸» prompt æ¨¡æ¿ã€‚
    # å ä½ç¬¦è¯´æ˜ï¼š
    #   {origin_db}           - æ¥æºæ•°æ®åº“åï¼Œç”¨äºè¯´æ˜æºè¯­æ³•/è¯­ä¹‰ä¸Šä¸‹æ–‡
    #   {target_db}           - ç›®æ ‡æ•°æ®åº“åï¼Œç”¨äºè¯´æ˜ç›®æ ‡è¯­æ³•è¦æ±‚
    #   {sql_statement}       - å¾…è½¬æ¢çš„åŸå§‹ SQLï¼ˆå·²é¢„å¤„ç†ï¼‰
    #   {feature_knowledge}   - æ³¨å…¥çš„ feature çŸ¥è¯†ï¼ˆæ˜ å°„/ç¤ºä¾‹/è§„åˆ™ï¼‰ï¼Œç›´æ¥å†™å…¥ prompt
    #   {examples}            - few-shot ç¤ºä¾‹ä¸²ï¼ˆå¯é€‰ï¼‰ï¼Œå¸®åŠ©æ¨¡å‹ä¸¾ä¾‹å­¦ä¹ 
    #   {format_instructions} - StructuredOutputParser ç»™å‡ºçš„è¾“å‡ºæ ¼å¼è¯´æ˜ï¼Œè¦æ±‚æ¨¡å‹æŒ‰ç»“æ„è¿”å›

    transfer_llm_string = """  
    You are an INTJ (MBTI) database engineering expert known for strategic, analytical, precise thinking. Let's think step by step. You are an expert in sql statement translation between different database.\
    With the assistance of feature knowledge,transfer the following {origin_db} statement to executable {target_db} statement with similar semantics.\
    {origin_db} statement: {sql_statement}\

    Transfer should ensure following requirements:
    1. All column names and feature variables remain unchanged.
    2. Strictly forbid meaningless features(such as NULL,0), features with random return value(such as current_time).
    3. Transfer as far as possible, and ensure similar semantics.\

    Transfer by carrying out following instructions step by step.\
    {feature_knowledge}\
    
    Check if transfer result satisfies requirements mentioned before.If not,modify the result.

    Here are some transfer examples: {examples}\
    Answer the following information: {format_instructions}
    """

    response_schemas = [
        ResponseSchema(
            type="string",
            name="TransferSQL",
            description="The transferred SQL statement result.",
        ),
        ResponseSchema(
            type="string",
            name="Explanation",
            description="Explain the basis for the conversion.",
        ),
    ]

    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
    format_instructions = output_parser.get_format_instructions()

    # FewShot = False ç›®å‰æ²¡æœ‰
    examples_string = get_examples_string(FewShot, origin_db, target_db)
    # examples_string = ""

    if with_knowledge:
        # ä¼˜å…ˆä½¿ç”¨ NoSQL knowledge (é€‚ç”¨äº Redisâ†’MongoDBã€SQLiteâ†’SurrealDB ç­‰åœºæ™¯)
        NOSQL_DBS = {"redis", "memcached", "etcd", "consul", "mongodb", "surrealdb"}
        if str(origin_db).lower() in NOSQL_DBS or str(target_db).lower() in NOSQL_DBS:
            feature_knowledge_string = get_NoSQL_knowledge_string(
                origin_db, target_db, with_knowledge, sql_statement_processed
            )
        else:
            # ä¼ ç»Ÿ SQL æ–¹è¨€æ˜ å°„çŸ¥è¯†
            feature_knowledge_string = get_feature_knowledge_string(
                origin_db,
                target_db,
                with_knowledge,
                test_info["SqlPotentialDialectFunctionMapping"],
                sql_statement_processed,
            )
    else:
        feature_knowledge_string = ""

    # ========== Mem0 å¢å¼º Prompt ==========
    if mem0_manager:
        try:
            enhanced_prompt = mem0_manager.build_enhanced_prompt(
                base_prompt=transfer_llm_string,
                query_sql=sql_statement_processed,
                origin_db=origin_db,
                target_db=target_db,
                include_knowledge_base=True  # ğŸ”¥ å¯ç”¨çŸ¥è¯†åº“å¢å¼º
            )
            # æ£€æŸ¥å¢å¼ºåçš„ prompt æ˜¯å¦å¼•å…¥äº†æ ¼å¼åŒ–é—®é¢˜
            # å°† Mem0 æ³¨å…¥çš„å†…å®¹ä¸­çš„å¤§æ‹¬å·è½¬ä¹‰
            print("ğŸ“š Prompt enhanced with Mem0 (historical knowledge + knowledge base)")
            transfer_llm_string = enhanced_prompt
        except Exception as e:
            print(f"âš ï¸ Failed to enhance prompt with Mem0: {e}")
            import traceback
            traceback.print_exc()

    # å®‰å…¨åœ°åˆ›å»º prompt templateï¼Œæ•è·æ ¼å¼åŒ–é”™è¯¯
    try:
        prompt_template = ChatPromptTemplate.from_template(transfer_llm_string)
    except Exception as e:
        print(f"âŒ Error creating prompt template: {e}")
        print(f"ğŸ” Problematic prompt snippet (first 500 chars):")
        print(transfer_llm_string[:500])
        print(f"ğŸ” Last 500 chars:")
        print(transfer_llm_string[-500:])
        # å°è¯•æ‰¾å‡ºæœ‰é—®é¢˜çš„å ä½ç¬¦
        import re
        potential_issues = re.findall(r'\{[^{}]+\}', transfer_llm_string)
        print(f"ğŸ” Found {len(potential_issues)} potential placeholder issues:")
        for issue in potential_issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   - {issue}")
        raise

    iterate_llm_string = """  
    The corresponding executable SQL statement that you provided in your most recent response resulted in an error when executed.\
    Please modify your most recent SQL statement response based on the error message.\
    Ensure that all column names remain unchanged between the sql statements before and after the transfer.\
    error message:{error_message}  
    Answer the following information: {format_instructions}  
    """

    iterate_response_schemas = [
        ResponseSchema(
            type="string",
            name="TransferSQL",
            description="The new transferred SQL statement result after modification.",
        ),
        ResponseSchema(
            type="string",
            name="Explanation",
            description="Explain the basis for the conversion and modification.",
        ),
    ]

    iterate_output_parser = StructuredOutputParser.from_response_schemas(
        iterate_response_schemas
    )
    iterate_format_instructions = iterate_output_parser.get_format_instructions()

    iterate_prompt_template = ChatPromptTemplate.from_template(iterate_llm_string)

    costs = []
    transfer_results = []
    exec_results = []
    exec_times = []
    error_messages = []
    exec_equalities = []
    # æ‰§è¡Œorigin sqlå¾—åˆ°ç»“æœï¼Œå¹¶å’Œå¾—åˆ°çš„æ‰€æœ‰transfer sqlç»“æœä¾æ¬¡è¿›è¡Œæ¯”å¯¹ï¼Œç¡®å®šæ‰§è¡Œç»“æœæ˜¯å¦ç›¸åŒï¼Œå°†å¯¹æ¯”ç»“æœå­˜å‚¨åˆ°exec_sameä¸­
    origin_exec_result, origin_exec_time, origin_error_message = exec_sql_statement(
        tool, exp, origin_db, sql_statement
    )

    conversation_cnt = 0  # conversation_cnt = 0:åˆå§‹ç¬¬ä¸€æ¡prompt

    # ========== ä¼˜å…ˆå°è¯• Transfer Agent (é’ˆå¯¹ NoSQL åœºæ™¯) ==========
    NOSQL_DBS = {"redis", "memcached", "etcd", "consul", "mongodb"}
    use_transfer_agent = os.environ.get(
        "QTRAN_TRANSFER_ENGINE", ""
    ).lower() == "agent" and (
        str(origin_db).lower() in NOSQL_DBS or str(target_db).lower() in NOSQL_DBS
    )
    print("use_transfer_agent: " + str(use_transfer_agent))
    if use_transfer_agent and conversation_cnt == 0:
        agent_result = _agent_transfer_statement(
            origin_db, target_db, sql_statement_processed
        )
        if agent_result and "TransferSQL" in agent_result:
            # Agent æˆåŠŸç”Ÿæˆè½¬æ¢
            transfer_sql = agent_result["TransferSQL"]
            explanation = agent_result.get("Explanation", "Generated by Transfer Agent")

            # æ‰§è¡Œè½¬æ¢åçš„ SQL
            exec_result, exec_time, error_message = exec_sql_statement(
                tool, exp, target_db, transfer_sql
            )

            # è®°å½•ç»“æœ
            transfer_results.append(agent_result)
            exec_results.append(str(exec_result))
            exec_times.append(str(exec_time))
            error_messages.append(str(error_message))
            costs.append({"Engine": "transfer_agent"})

            # åˆ¤æ–­æ˜¯å¦æˆåŠŸ
            if error_message == "None" or not error_message:
                # æ‰§è¡ŒæˆåŠŸï¼Œè®¡ç®—ç­‰ä»·æ€§
                exec_equality = str(origin_exec_result) == str(exec_result)
                exec_equalities.append(exec_equality)
                # æˆåŠŸåˆ™ç›´æ¥è¿”å›
                return (
                    costs,
                    transfer_results,
                    exec_results,
                    exec_times,
                    error_messages,
                    str(origin_exec_result),
                    str(origin_exec_time),
                    str(origin_error_message),
                    exec_equalities,
                )
            else:
                # æ‰§è¡Œå¤±è´¥ï¼Œè®°å½•ç­‰ä»·æ€§ä¸º Falseï¼Œç»§ç»­ä¼ ç»Ÿè¿­ä»£
                print("Agent è½¬æ¢æˆåŠŸï¼Œä½†æ‰§è¡Œå¤±è´¥ï¼Œè¿›å…¥ä¼ ç»Ÿ LLM è¿­ä»£ä¿®æ­£")
                exec_equalities.append(False)
                conversation_cnt = 1  # è¿›å…¥è¿­ä»£ä¿®æ­£
        else:
            # Agent å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿ LLM
            print("Agent å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿ LLM")
            use_transfer_agent = False

    # ========== ä¼ ç»Ÿ LLM è½¬æ¢è·¯å¾„ ==========
    # è¾¹ç•Œ1ï¼šè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    while conversation_cnt <= iteration_num:
        prompt_messages = None
        if conversation_cnt == 0:
            # åˆå§‹ç¬¬ä¸€æ¡prompt
            prompt_messages = prompt_template.format_messages(
                origin_db=origin_db,
                target_db=target_db,
                sql_statement=sql_statement_processed,
                examples=examples_string,
                feature_knowledge=feature_knowledge_string,
                format_instructions=format_instructions,
            )
        else:
            # è¾¹ç•Œ2ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦è¿­ä»£ï¼Œä¸éœ€è¦è¿­ä»£åˆ™breakè·³å‡ºwhileå¾ªç¯
            print(error_messages)
            if error_iteration is False:
                break
            # è¾¹ç•Œ3ï¼šåˆ¤æ–­ä¸Šä¸€æ¬¡å¾—åˆ°çš„transfer sqlæ‰§è¡Œæ˜¯å¦æ‰§è¡ŒæˆåŠŸï¼Œèƒ½æ‰§è¡ŒæˆåŠŸåˆ™breakç›´æ¥è·³å‡ºå¾ªç¯ï¼Œæ‰§è¡Œå¤±è´¥åˆ™è¿›è¡Œä¸‹é¢çš„errorä¿¡æ¯è¿­ä»£å¤„ç†
            if len(error_messages) and error_messages[-1] == "None":
                break

            # errorä¿¡æ¯è¿­ä»£å¤„ç†ï¼š
            # é’ˆå¯¹æ‰§è¡Œå¤±è´¥çš„transfer sqlï¼Œå°†æ‰§è¡Œçš„errorä¿¡æ¯è¿”å›ç»™llmï¼Œæç¤ºllmæ ¹æ®errorä¿¡æ¯é‡æ–°ç”Ÿæˆæ–°çš„transfer sqlï¼Œåå¤è¿­ä»£
            # æ­¤å¤„è¿ç”¨äº†ConversationBufferMemory()ï¼Œå¹¶ä¸”è§„å®šäº†æœ€é«˜è¿­ä»£æ¬¡æ•°ä¸º iteration_num

            # è¿­ä»£çš„promptï¼šæ¯æ¬¡å–error_messagesæ•°ç»„çš„æœ€åä¸€ä¸ªå…ƒç´ ï¼Œåˆ™ä¸ºæœ€æ–°çš„æŠ¥é”™ä¿¡æ¯
            prompt_messages = iterate_prompt_template.format_messages(
                error_message=error_messages[-1] if len(error_messages) else "",
                format_instructions=iterate_format_instructions,
            )

        """
        try:

        except Exception as e:
            print("transfer llm:")
            print(e)
        """
        cost = {}
        with get_openai_callback() as cb:
            print("Prompt_messages: " + prompt_messages[0].content)
            # print("Prompt_messages: " + prompt_messages[0].content)
            response = conversation.predict(input=prompt_messages[0].content)
            print("output_dict_response: " + response)
            
            # ä½¿ç”¨ json_repair ä¿®å¤å¯èƒ½çš„JSONæ ¼å¼é”™è¯¯
            try:
                output_dict = output_parser.parse(response)
            except (TypeError, json.JSONDecodeError, Exception) as e:
                print(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ json_repair ä¿®å¤: {e}")
                try:
                    from json_repair import repair_json
                    # æå–JSONéƒ¨åˆ†ï¼ˆå»é™¤markdownä»£ç å—æ ‡è®°ï¼‰
                    json_text = response
                    if "```json" in response:
                        json_text = response.split("```json")[1].split("```")[0].strip()
                    elif "```" in response:
                        json_text = response.split("```")[1].split("```")[0].strip()
                    
                    # ä½¿ç”¨ repair_json ä¿®å¤
                    repaired = repair_json(json_text)
                    output_dict = json.loads(repaired)
                    
                    # æ•°æ®ç»“æ„æ ¡æ­£ï¼šå¦‚æœæ˜¯åˆ—è¡¨ï¼Œæå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    if isinstance(output_dict, list):
                        if len(output_dict) > 0 and isinstance(output_dict[0], dict):
                            print(f"âš ï¸ JSONä¿®å¤åæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè‡ªåŠ¨æå–ç¬¬ä¸€ä¸ªå…ƒç´ ")
                            output_dict = output_dict[0]
                        else:
                            print(f"âŒ JSONä¿®å¤åçš„åˆ—è¡¨æ ¼å¼ä¸æ­£ç¡®")
                            return None, None, None, None, {"error": "Invalid JSON structure"}, "JSON_STRUCTURE_ERROR"
                    
                    # éªŒè¯å¿…éœ€çš„keyæ˜¯å¦å­˜åœ¨
                    if not isinstance(output_dict, dict):
                        print(f"âŒ JSONä¿®å¤åä¸æ˜¯å­—å…¸æ ¼å¼: {type(output_dict)}")
                        return None, None, None, None, {"error": "Not a dict"}, "JSON_STRUCTURE_ERROR"
                    
                    if "TransferSQL" not in output_dict:
                        print(f"âŒ JSONä¿®å¤åç¼ºå°‘ 'TransferSQL' å­—æ®µ")
                        return None, None, None, None, {"error": "Missing TransferSQL"}, "JSON_STRUCTURE_ERROR"
                    
                    print(f"âœ… JSONä¿®å¤æˆåŠŸ")
                except Exception as repair_error:
                    print(f"âŒ JSONä¿®å¤å¤±è´¥: {repair_error}")
                    print(f"åŸå§‹å“åº”:\n{response}")
                    # è¿”å›é”™è¯¯æ ‡è®°ï¼Œè®©åç»­æµç¨‹å¤„ç†
                    return None, None, None, None, {"error": str(e)}, "JSON_PARSE_ERROR"
            
            # print(response)
            print("output_dict: " + str(output_dict))
            cost["Total Tokens"] = cb.total_tokens
            cost["Prompt Tokens"] = cb.prompt_tokens
            cost["Completion Tokens"] = cb.completion_tokens
            cost["Total Cost (USD)"] = (
                cb.total_cost
            )  # ç”¨äº†4o-miniä»¥åå˜æˆ0.0äº†ï¼Œè¿˜æ²¡ä¿®æ”¹ï¼Œä¹Ÿå¯ä»¥ç”¨æˆ·tokenä¹˜å•ä»·è®¡ç®—

            exec_result, exec_time, error_message = exec_sql_statement(
                tool, exp, target_db, output_dict["TransferSQL"]
            )
            costs.append(cost)
            transfer_results.append(output_dict)
            exec_results.append(str(exec_result))
            exec_times.append(str(exec_time))
            # ç®€åŒ–error_message,åªç•™ä¸‹å…³é”®éƒ¨åˆ†
            """
            if error_message:
                error_message = error_message.split(":")[1]
            """
            print(error_messages)
            error_messages.append(str(error_message))
            if (
                not error_message
                and not origin_error_message
                and exec_result == origin_exec_result
            ):
                exec_equalities.append(True)
            else:
                exec_equalities.append(False)
        
        # ========== Mem0 è®°å½•ç¿»è¯‘ç»“æœ ==========
        if mem0_manager and conversation_cnt > 0:
            try:
                # è®°å½•æˆåŠŸçš„ç¿»è¯‘
                if error_messages and error_messages[-1] == "None":
                    features = test_info.get("SqlPotentialDialectFunction", [])
                    mem0_manager.record_successful_translation(
                        origin_sql=sql_statement,
                        target_sql=output_dict.get("TransferSQL", ""),
                        origin_db=origin_db,
                        target_db=target_db,
                        iterations=conversation_cnt,
                        features=features
                    )
                    print(f"ğŸ’¾ Recorded successful translation (iteration {conversation_cnt})")
                
                # è®°å½•é”™è¯¯ä¿®æ­£ï¼ˆä»å¤±è´¥åˆ°æˆåŠŸçš„è½¬å˜ï¼‰
                if conversation_cnt > 1 and len(error_messages) >= 2:
                    if error_messages[-2] != "None" and error_messages[-1] == "None":
                        mem0_manager.record_error_fix(
                            error_message=error_messages[-2],
                            fix_sql=output_dict.get("TransferSQL", ""),
                            origin_db=origin_db,
                            target_db=target_db,
                            failed_sql=transfer_results[-2].get("TransferSQL", "") if len(transfer_results) >= 2 else ""
                        )
                        print(f"ğŸ”§ Recorded error fix pattern")
            except Exception as e:
                print(f"âš ï¸ Failed to record to Mem0: {e}")
        
        conversation_cnt += 1

    # ========== Mem0 ä¼šè¯ç»“æŸ ==========
    if mem0_manager:
        try:
            success = len(error_messages) > 0 and error_messages[-1] == "None"
            final_result = transfer_results[-1].get("TransferSQL", "") if transfer_results else None
            mem0_manager.end_session(success=success, final_result=final_result)
            
            # æ‰“å°æ€§èƒ½æŒ‡æ ‡
            if hasattr(mem0_manager, 'get_metrics_report'):
                print(mem0_manager.get_metrics_report())
        except Exception as e:
            print(f"âš ï¸ Failed to end Mem0 session: {e}")

    return (
        costs,
        transfer_results,
        exec_results,
        exec_times,
        error_messages,
        str(origin_exec_result),
        str(origin_exec_time),
        str(origin_error_message),
        exec_equalities,
    )


# NoSQL Crash/Hang åˆ†æ”¯ï¼šå½“ä»»ä¸€ç«¯æ¶‰åŠ NoSQL æ—¶ä¸åšè¯­ä¹‰ç­‰ä»·åˆ¤æ–­ï¼Œæ”¹ç”¨ crash pipeline æ£€æµ‹ç¨³å®šæ€§ã€‚
def transfer_llm_nosql_crash(
    tool,
    exp,
    conversation,  # æœªä½¿ç”¨ï¼Œå ä½ä¿æŒæ¥å£ä¸€è‡´
    error_iteration,
    iteration_num,
    FewShot,
    with_knowledge,
    origin_db,
    target_db,
    test_info,
    use_redis_kb: bool = False,
):

    SQL_DIALECTS = {
        "mysql",
        "mariadb",
        "tidb",
        "postgres",
        "sqlite",
        "duckdb",
        "clickhouse",
        "monetdb",
    }
    NOSQL_DBS = {"redis", "memcached", "etcd", "consul", "mongodb", "surrealdb"}

    # é€‰æ‹©è¦æ‰§è¡Œ crash æ£€æµ‹çš„å…·ä½“ NoSQL æ•°æ®åº“ (ä¼˜å…ˆ target)
    nosql_db = None
    if str(target_db).lower() in NOSQL_DBS:
        nosql_db = target_db
    elif str(origin_db).lower() in NOSQL_DBS:
        nosql_db = origin_db
    else:
        raise ValueError("transfer_llm_nosql_crash called but neither db is NoSQL")

    # æ„å»º prompt
    raw_statement = test_info.get("sqls", "") or ""
    # æ³¨å…¥çŸ¥è¯†/ç¤ºä¾‹
    feature_knowledge_string = ""
    if str(origin_db).lower() == "redis":
        feature_knowledge_string = get_NoSQL_knowledge_string(
            origin_db, target_db, with_knowledge, raw_statement
        )
    # FewShot ç¤ºä¾‹ï¼ˆå¯é€‰ï¼Œæš‚ä¸å®ç°ï¼‰
    examples_string = ""

    # ä½¿ç”¨å ä½ç¬¦é¿å… feature_knowledge_string / examples_string ä¸­çš„å¤§æ‹¬å·è¢«äºŒæ¬¡è§£æå¯¼è‡´ KeyError
    transfer_llm_string = """
    You are an INTJ (MBTI) database engineering expert known for strategic, analytical, precise thinking. You are an expert in NoSQL command translation and robustness testing.\
    Given the following SQL or pseudo-SQL, generate an equivalent {nosql_db} command or sequence.\
    Input statement: {sql_statement}\
    {feature_knowledge}\
    Requirements:\n1. Output only valid {nosql_db} commands (one per line if multiple).\n2. Do not invent keys/fields not present in the input.\n3. If the input is already a {nosql_db} command, output as-is.\n4. If you are unsure, make a best effort and explain.\n\n{examples}\nAnswer the following information: {format_instructions}\n""".replace(
        "{nosql_db}", nosql_db
    )

    response_schemas = [
        ResponseSchema(
            type="string",
            name="TransferNoSQL",
            description="The generated NoSQL command(s) (one per line if multiple).",
        ),
        ResponseSchema(
            type="string",
            name="Explanation",
            description="Explain the mapping or transformation.",
        ),
    ]
    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
    format_instructions = output_parser.get_format_instructions()
    prompt_template = ChatPromptTemplate.from_template(transfer_llm_string)

    iterate_llm_string = f"""
    The NoSQL command(s) you provided failed to execute robustly (crash/hang/error).\
    Please revise your previous command(s) based on the following error or event:\n{{error_message}}\n\nRequirements: Output only valid {nosql_db} command(s), one per line.\nAnswer the following information: {{format_instructions}}\n"""
    iterate_response_schemas = [
        ResponseSchema(
            type="string",
            name="TransferNoSQL",
            description="The revised NoSQL command(s) after error correction.",
        ),
        ResponseSchema(
            type="string",
            name="Explanation",
            description="Explain the correction.",
        ),
    ]
    iterate_output_parser = StructuredOutputParser.from_response_schemas(
        iterate_response_schemas
    )
    iterate_format_instructions = iterate_output_parser.get_format_instructions()
    iterate_prompt_template = ChatPromptTemplate.from_template(iterate_llm_string)

    costs = []
    transfer_results = []
    exec_results = []
    exec_times = []
    error_messages = []
    exec_equalities = []

    # è¿­ä»£ç”Ÿæˆ+æ‰§è¡Œ+æ£€æµ‹
    max_iter = iteration_num
    conversation_cnt = 0
    last_cmds = None
    last_explanation = None
    last_error = None
    while conversation_cnt <= max_iter:
        if conversation_cnt == 0:
            prompt_messages = prompt_template.format_messages(
                sql_statement=raw_statement,
                feature_knowledge=feature_knowledge_string,
                examples=examples_string,
                format_instructions=format_instructions,
            )
        else:
            if error_iteration is False:
                break
            if len(error_messages) and error_messages[-1] == "None":
                break
            prompt_messages = iterate_prompt_template.format_messages(
                error_message=last_error or "",
                format_instructions=iterate_format_instructions,
            )
        cost = {}
        with get_openai_callback() as cb:
            response = conversation.predict(input=prompt_messages[0].content)
            
            # ä½¿ç”¨ json_repair ä¿®å¤å¯èƒ½çš„JSONæ ¼å¼é”™è¯¯
            try:
                output_dict = (
                    output_parser.parse(response)
                    if conversation_cnt == 0
                    else iterate_output_parser.parse(response)
                )
            except (TypeError, json.JSONDecodeError, Exception) as e:
                print(f"âš ï¸ JSONè§£æå¤±è´¥ (è¿­ä»£{conversation_cnt}æ¬¡)ï¼Œå°è¯•ä½¿ç”¨ json_repair ä¿®å¤: {e}")
                try:
                    from json_repair import repair_json
                    # æå–JSONéƒ¨åˆ†
                    json_text = response
                    if "```json" in response:
                        json_text = response.split("```json")[1].split("```")[0].strip()
                    elif "```" in response:
                        json_text = response.split("```")[1].split("```")[0].strip()
                    
                    # ä½¿ç”¨ repair_json ä¿®å¤
                    repaired = repair_json(json_text)
                    output_dict = json.loads(repaired)
                    
                    # æ•°æ®ç»“æ„æ ¡æ­£ï¼šå¦‚æœæ˜¯åˆ—è¡¨ï¼Œæå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    if isinstance(output_dict, list):
                        if len(output_dict) > 0 and isinstance(output_dict[0], dict):
                            print(f"âš ï¸ JSONä¿®å¤åæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè‡ªåŠ¨æå–ç¬¬ä¸€ä¸ªå…ƒç´  (è¿­ä»£{conversation_cnt}æ¬¡)")
                            output_dict = output_dict[0]
                        else:
                            print(f"âŒ JSONä¿®å¤åçš„åˆ—è¡¨æ ¼å¼ä¸æ­£ç¡® (è¿­ä»£{conversation_cnt}æ¬¡)")
                            output_dict = {"TransferNoSQL": "", "Explanation": f"JSON_STRUCTURE_ERROR"}
                    
                    # éªŒè¯å¿…éœ€çš„key
                    if isinstance(output_dict, dict):
                        if "TransferNoSQL" not in output_dict:
                            print(f"âŒ JSONä¿®å¤åç¼ºå°‘ 'TransferNoSQL' å­—æ®µ (è¿­ä»£{conversation_cnt}æ¬¡)")
                            output_dict["TransferNoSQL"] = ""
                    
                    print(f"âœ… JSONä¿®å¤æˆåŠŸ (è¿­ä»£{conversation_cnt}æ¬¡)")
                except Exception as repair_error:
                    print(f"âŒ JSONä¿®å¤å¤±è´¥: {repair_error}")
                    print(f"åŸå§‹å“åº”:\n{response}")
                    # æ ‡è®°é”™è¯¯å¹¶ç»§ç»­ï¼Œè®©åç»­é”™è¯¯å¤„ç†æµç¨‹å¤„ç†
                    output_dict = {"TransferNoSQL": "", "Explanation": f"JSON_PARSE_ERROR: {str(e)}"}
            
            cost["Total Tokens"] = cb.total_tokens
            cost["Prompt Tokens"] = cb.prompt_tokens
            cost["Completion Tokens"] = cb.completion_tokens
            cost["Total Cost (USD)"] = cb.total_cost
        # æ‹†åˆ†å‘½ä»¤
        cmds = [
            c.strip() for c in output_dict["TransferNoSQL"].split("\n") if c.strip()
        ]
        last_cmds = cmds
        last_explanation = output_dict["Explanation"]
        # æ‰§è¡Œ crash/hang æ£€æµ‹
        start_batch = time.time()
        pipeline_res = run_nosql_sequence(
            nosql_db, cmds, sequence_id=str(test_info.get("index", "unknown"))
        )
        duration_batch = time.time() - start_batch
        exec_results.append(json.dumps(pipeline_res, ensure_ascii=False))
        exec_times.append(str(duration_batch))
        transfer_results.append(output_dict)
        costs.append(cost)
        # error å®šä¹‰
        if pipeline_res.get("crash"):
            error_messages.append("crash")
            last_error = "crash: " + str(pipeline_res.get("first_failure"))
        elif pipeline_res.get("hang"):
            error_messages.append("hang")
            last_error = "hang: " + str(pipeline_res.get("first_failure"))
        elif pipeline_res["summary"]["errors"] > 0:
            error_messages.append("error")
            last_error = "error: " + str(pipeline_res.get("first_failure"))
        else:
            error_messages.append("None")
            last_error = None
        # åªè¦æœ‰ crash/hang/error å°±ç»§ç»­è¿­ä»£ï¼Œå¦åˆ™ç»ˆæ­¢
        if last_error is None:
            exec_equalities.append(True)
            break
        else:
            exec_equalities.append(False)
        conversation_cnt += 1

    # origin æ‰§è¡Œç»“æœå ä½ï¼šå¯é€‰æ‰§è¡Œä¸€æ¬¡ï¼ˆè‹¥ origin ä¸º SQL ä¸” target ä¸º NoSQL åˆ™æ‰§è¡Œ origin ä»¥ç•™å­˜åŸºçº¿ï¼‰
    origin_exec_result = ""
    origin_exec_time = ""
    origin_error_message = ""
    try:
        from src.Tools.DatabaseConnect.database_connector import (
            exec_sql_statement as _exec,
        )

        if str(origin_db).lower() in SQL_DIALECTS | NOSQL_DBS:
            r, t, e = _exec(tool, exp, origin_db, raw_statement)
            origin_exec_result, origin_exec_time, origin_error_message = r, t, e
    except Exception:
        pass

    return (
        costs,
        transfer_results,
        exec_results,
        exec_times,
        error_messages,
        str(origin_exec_result),
        str(origin_exec_time),
        str(origin_error_message),
        exec_equalities,
    )


def transfer_llm(
    tool,
    exp,
    conversation,
    error_iteration,
    iteration_num,
    FewShot,
    with_knowledge,
    origin_db,
    target_db,
    test_info,
    use_redis_kb: bool = False,
):
    """è°ƒåº¦å…¥å£:
    æ ¹æ®æµ‹è¯•ç­–ç•¥(molt)å†³å®šä½¿ç”¨å“ªç§æµ‹è¯•æ–¹æ³•:
    - semantic/norec/tlp/dqe/pinolo ç­‰è¯­ä¹‰ç­–ç•¥: ä½¿ç”¨è¯­ä¹‰ç­‰ä»·æµ‹è¯• (transfer_llm_sql_semantic)
    - crash/hang/fuzz ç­‰ç¨³å®šæ€§ç­–ç•¥: ä½¿ç”¨å´©æºƒ/æŒ‚èµ·æ£€æµ‹ (transfer_llm_nosql_crash)

    åˆ¤æ–­ä¾æ®: test_info ä¸­çš„ 'molt' å­—æ®µ(æµ‹è¯•å·¥å…·/ç­–ç•¥),è€Œéæ•°æ®åº“ç±»å‹
    è¿”å›å€¼æ ¼å¼ä¸æ—§ç‰ˆä¿æŒä¸€è‡´ã€‚
    """
    # ä» test_info ä¸­è·å–æµ‹è¯•ç­–ç•¥
    molt = test_info.get("molt", "").lower()

    # è¯­ä¹‰ç­‰ä»·æµ‹è¯•ç­–ç•¥(SQLancer/Pinolo çš„å„ç§ oracle)
    SEMANTIC_STRATEGIES = {
        "semantic",  # è¯­ä¹‰ç­‰ä»·
        "norec",  # NoREC oracle
        "tlp",  # TLP (Ternary Logic Partitioning)
        "dqe",  # Differential Query Execution
        "pinolo",  # Pinolo
        "pqs",  # Pivoted Query Synthesis
    }

    # å´©æºƒ/ç¨³å®šæ€§æµ‹è¯•ç­–ç•¥
    CRASH_STRATEGIES = {
        "crash",  # å´©æºƒæ£€æµ‹
        "hang",  # æŒ‚èµ·æ£€æµ‹
        "fuzz",  # æ¨¡ç³Šæµ‹è¯•
        "stress",  # å‹åŠ›æµ‹è¯•
    }

    # æ ¹æ® molt å†³å®šæµ‹è¯•ç­–ç•¥
    if molt in SEMANTIC_STRATEGIES:
        # ä½¿ç”¨è¯­ä¹‰ç­‰ä»·æµ‹è¯•(é€‚ç”¨äº SQL æˆ– NoSQL,åªè¦ç­–ç•¥æ˜¯è¯­ä¹‰ç›¸å…³çš„)
        return transfer_llm_sql_semantic(
            tool,
            exp,
            conversation,
            error_iteration,
            iteration_num,
            FewShot,
            with_knowledge,
            origin_db,
            target_db,
            test_info,
            use_redis_kb=use_redis_kb,
        )
    elif molt in CRASH_STRATEGIES:
        # ä½¿ç”¨å´©æºƒ/æŒ‚èµ·æ£€æµ‹(é€‚ç”¨äºç¨³å®šæ€§æµ‹è¯•)
        return transfer_llm_nosql_crash(
            tool,
            exp,
            conversation,
            error_iteration,
            iteration_num,
            FewShot,
            with_knowledge,
            origin_db,
            target_db,
            test_info,
            use_redis_kb=use_redis_kb,
        )
    else:
        # é»˜è®¤ç­–ç•¥: å¦‚æœ molt æœªè¯†åˆ«,æ ¹æ®æ•°æ®åº“ç±»å‹å›é€€
        # è¿™æ˜¯ä¸ºäº†å‘åå…¼å®¹æ²¡æœ‰ molt å­—æ®µçš„æ—§æ•°æ®
        SQL_DIALECTS = {
            "mysql",
            "mariadb",
            "tidb",
            "postgres",
            "sqlite",
            "duckdb",
            "clickhouse",
            "monetdb",
        }
        NOSQL_DBS = {"redis", "memcached", "etcd", "consul", "mongodb"}

        if (str(origin_db).lower() in SQL_DIALECTS) and (
            str(target_db).lower() in SQL_DIALECTS
        ):
            # SQL â†’ SQL: é»˜è®¤ä½¿ç”¨è¯­ä¹‰æµ‹è¯•
            return transfer_llm_sql_semantic(
                tool,
                exp,
                conversation,
                error_iteration,
                iteration_num,
                FewShot,
                with_knowledge,
                origin_db,
                target_db,
                test_info,
                use_redis_kb=use_redis_kb,
            )
        else:
            # æ¶‰åŠ NoSQL
            return transfer_llm_nosql_crash(
                tool,
                exp,
                conversation,
                error_iteration,
                iteration_num,
                FewShot,
                with_knowledge,
                origin_db,
                target_db,
                test_info,
                use_redis_kb=use_redis_kb,
            )


def pinolo_qtran_run(
    input_filename,
    tool,
    temperature=0.3,
    model="gpt-4o-mini",
    error_iteration=True,
    iteration_num=4,
    FewShot=False,
    with_knowledge=True,
):
    return


def get_feature_knowledge(value, feature_type):
    if feature_type == "function":
        mapping_indexes = value["SqlPotentialDialectFunctionMapping"]


def transfer_data(
    tool,
    temperature,
    model,
    error_iteration,
    iteration_num,
    FewShot,
    with_knowledge,
    output_name,
    origin_db_name,
    target_db_name,
    len_low,
    len_high,
    IsRandom,
    num,
    sqls_type,
):
    """
    # transfer dataï¼šå°†ç»è¿‡init_data()å¤„ç†çš„åˆå§‹åŒ–åæ•°æ®ï¼Œé€æ¡è¿›è¡Œtransfer llmçš„è½¬æ¢å¹¶å­˜å‚¨ç»“æœ
    # æºæ–‡ä»¶ï¼šPinolo Output/output_testä¸‹ALLå’ŒRANDOMæ–‡ä»¶å¤¹å†…çš„init_output1_mariadb_x_x_originalSql_all.json,init_output1_mariadb_x_x_originalSqlsim_all.json
    # ç›®æ ‡æ–‡ä»¶ï¼šPinolo Output/output_test_resultsä¸‹ALLå’ŒRANDOMæ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶
    """

    results_filename = None
    sqls_filename = None
    if IsRandom:
        sql_temp = (
            output_name
            + "_"
            + origin_db_name
            + "_to_"
            + target_db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSql_random_"
            + str(num)
            + ".jsonl"
        )
        sqlsim_temp = (
            output_name
            + "_"
            + origin_db_name
            + "_to_"
            + target_db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlsim_random_"
            + str(num)
            + ".jsonl"
        )
        res_sql_temp = (
            output_name
            + "_"
            + origin_db_name
            + "_to_"
            + target_db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSql_random_"
            + str(num)
            + ".jsonl"
        )
        res_sqlsim_temp = (
            output_name
            + "_"
            + origin_db_name
            + "_to_"
            + target_db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlsim_random_"
            + str(num)
            + ".jsonl"
        )
        if FewShot:
            res_sql_temp = "fewshot_" + res_sql_temp
            res_sqlsim_temp = "fewshot_" + res_sqlsim_temp
        if error_iteration:
            res_sql_temp = "iterated_" + res_sql_temp
            res_sqlsim_temp = "iterated_" + res_sqlsim_temp
        if sqls_type == "origin":
            sqls_filename = os.path.join(
                "..\..\Output\TransferLLM\Pinolo",
                "PotentialDialectFeatures",
                "RANDOM",
                "init_" + sql_temp,
            )
            results_filename = os.path.join(
                "..\..\Output\TransferLLM\Pinolo",
                "Results_With_Feature_Knowledge",
                "RANDOM",
                res_sql_temp,
            )
        elif sqls_type == "simple":
            sqls_filename = os.path.join(
                "..\..\Output\TransferLLM\Pinolo",
                "PotentialDialectFeatures",
                "RANDOM",
                "init_" + sqlsim_temp,
            )
            results_filename = os.path.join(
                "..\..\Output\TransferLLM\Pinolo",
                "Results_With_Feature_Knowledge",
                "RANDOM",
                res_sqlsim_temp,
            )
    else:
        sql_temp = (
            output_name
            + "_"
            + origin_db_name
            + "_to_"
            + target_db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSql_all_"
            + str(num)
            + ".jsonl"
        )
        sqlsim_temp = (
            output_name
            + "_"
            + origin_db_name
            + "_to_"
            + target_db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlsim_all_"
            + str(num)
            + ".jsonl"
        )
        res_sql_temp = (
            output_name
            + "_"
            + origin_db_name
            + "_to_"
            + target_db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSql_all.jsonl"
        )
        res_sqlsim_temp = (
            output_name
            + "_"
            + origin_db_name
            + "_to_"
            + target_db_name
            + "_"
            + str(len_low)
            + "_"
            + str(len_high)
            + "_originalSqlsim_all.jsonl"
        )
        if FewShot:
            res_sql_temp = "fewshot_" + res_sql_temp
            res_sqlsim_temp = "fewshot_" + res_sqlsim_temp
        if error_iteration:
            res_sql_temp = "iterated_" + res_sql_temp
            res_sqlsim_temp = "iterated_" + res_sqlsim_temp
        if sqls_type == "origin":
            sqls_filename = os.path.join(
                "..\..\Output\TransferLLM\Pinolo",
                "PotentialDialectFeatures",
                "ALL",
                "init_" + sql_temp,
            )
            results_filename = os.path.join(
                "..\..\Output\TransferLLM\Pinolo",
                "Results_With_Feature_Knowledge",
                "ALL",
                res_sql_temp,
            )
        elif sqls_type == "simple":
            sqls_filename = os.path.join(
                "..\..\Output\TransferLLM\Pinolo",
                "PotentialDialectFeatures",
                "ALL",
                "init_" + sqlsim_temp,
            )
            results_filename = os.path.join(
                "..\..\Output\TransferLLM\Pinolo",
                "Results_With_Feature_Knowledge",
                "ALL",
                res_sqlsim_temp,
            )

    processed_cnt = 0
    if os.path.exists(results_filename):
        with open(results_filename, "r", encoding="utf-8") as rf:
            processed_cnt = len(rf.readlines())

    with open(sqls_filename, "r", encoding="utf-8") as rf:
        Sqls = rf.readlines()

    for index in range(len(Sqls)):
        # for index in range(30):
        value = json.loads(Sqls[index])
        print(index)
        if value["index"] < processed_cnt:
            continue
        print(value)
        (
            costs,
            transfer_results,
            exec_results,
            exec_times,
            error_messages,
            origin_exec_result,
            origin_exec_time,
            origin_error_message,
            exec_equalities,
        ) = transfer_llm(
            tool=tool,
            temperature=temperature,
            model=model,
            error_iteration=error_iteration,
            iteration_num=iteration_num,
            FewShot=FewShot,
            with_knowledge=with_knowledge,
            origin_db=origin_db_name,
            target_db=target_db_name,
            test_info=value,
        )

        value["SqlExecResult"] = origin_exec_result
        value["SqlExecTime"] = origin_exec_time
        value["SqlExecError"] = origin_error_message

        value["TransferResult"] = transfer_results
        value["TransferCost"] = costs
        value["TransferSqlExecResult"] = exec_results
        value["TransferSqlExecTime"] = exec_times
        value["TransferSqlExecError"] = (
            error_messages  # è®°å½•transfer sqlçš„å¤šæ¬¡æ‰§è¡Œæ˜¯å¦æœ‰æŠ¥é”™ï¼šå¦‚æœä¸ºNoneåˆ™è¡¨ç¤ºæˆåŠŸæ‰§è¡Œï¼Œå¦‚æœä¸æ˜¯Noneè€Œæ˜¯å…·ä½“æŠ¥é”™åˆ™è¯´æ˜æ‰§è¡Œå¤±è´¥
        )
        value["TransferSqlExecEqualities"] = (
            exec_equalities  # è®°å½•transfer sqlçš„å¤šæ¬¡æ‰§è¡Œç»“æœæ˜¯å¦ä¸origin sqlä¸€æ ·ï¼šå¦‚æœä¸ºTrueåˆ™è¯´æ˜ä¸€æ ·ï¼Œå¦åˆ™ä¸ºä¸ä¸€æ ·
        )

        with open(results_filename, "a", encoding="utf-8") as a:
            json.dump(value, a)
            a.write("\n")

        try:
            a = 1
        except Exception as e:
            print(e)


# sql_type:"origin"æˆ–"simple"ï¼Œåˆ†åˆ«ä»£è¡¨åŸå§‹sqlå’Œç®€åŒ–åçš„sql
def exec_transfer_llm(
    temperature,
    model,
    error_iteration,
    iteration_num,
    FewShot,
    with_knowledge,
    output_name,
    origin_db_name,
    target_db_name,
    len_low,
    len_high,
    IsRandom,
    num,
    sqls_type,
):
    load_data(output_name, origin_db_name, len_low, len_high, IsRandom, num)
    init_data(output_name, origin_db_name, len_low, len_high, IsRandom, num)
    transfer_data(
        "pinolo",
        temperature,
        model,
        error_iteration,
        iteration_num,
        FewShot,
        with_knowledge,
        output_name,
        origin_db_name,
        target_db_name,
        len_low,
        len_high,
        IsRandom,
        num,
        sqls_type,
    )
