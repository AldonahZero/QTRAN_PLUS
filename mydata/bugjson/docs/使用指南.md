# æ•°æ®åº“Bugçˆ¬è™« - å®Œæ•´ä½¿ç”¨æŒ‡å—

## ğŸ“‹ é—®é¢˜è§£å†³

### âœ… å·²è§£å†³ï¼šçˆ¬å–æ•°æ®"ä¹±ä¸ƒå…«ç³Ÿ"çš„é—®é¢˜

**åŸå› **ï¼š
- ä»GitHub issuesä¸­æå–çš„å†…å®¹åŒ…å«æ—¥å¿—ã€å †æ ˆè·Ÿè¸ªã€è¡¨æ ¼è¾¹æ¡†ç­‰å™ªéŸ³
- SQLè¯­å¥æ··æ‚åœ¨é”™è¯¯ä¿¡æ¯ä¸­

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. âœ… æ”¹è¿›äº†SQLæå–ç®—æ³•ï¼ˆ`advanced_crawler.py`ï¼‰
2. âœ… æ·»åŠ äº†å™ªéŸ³è¿‡æ»¤é€»è¾‘ï¼ˆè¿‡æ»¤æ—¥å¿—ã€å †æ ˆè·Ÿè¸ªã€è¡¨æ ¼è¾¹æ¡†ç­‰ï¼‰
3. âœ… åˆ›å»ºäº†æ•°æ®æ¸…ç†å·¥å…·ï¼ˆ`clean_bugs.py`ï¼‰

---

## ğŸš€ ä¸‰æ­¥å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1ï¼šæ¸…ç†å·²æœ‰æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰

```bash
cd /root/QTRAN/mydata/bugjson

# æ¸…ç†bugs.jsonï¼ˆä¼šè‡ªåŠ¨å¤‡ä»½ï¼‰
python clean_bugs.py bugs.json

# æ¸…ç†æ¼”ç¤ºæ•°æ®
python clean_bugs.py bugs_demo.json
```

**æ•ˆæœ**ï¼š
- âœ… è‡ªåŠ¨å¤‡ä»½åŸæ–‡ä»¶åˆ° `bugs.json.backup.dirty`
- âœ… ç§»é™¤testå­—æ®µä¸­çš„æ—¥å¿—ã€é”™è¯¯ä¿¡æ¯
- âœ… åªä¿ç•™çº¯å‡€çš„SQLè¯­å¥
- âœ… ç§»é™¤æ— å…³çš„commentå†…å®¹

### æ­¥éª¤2ï¼šé…ç½®GitHub Tokenï¼ˆæ¨èï¼‰

```bash
# è·å–Token: https://github.com/settings/tokens
# éœ€è¦æƒé™: public_repo

# æ–¹å¼1: ç›´æ¥ç¼–è¾‘é…ç½®æ–‡ä»¶
vim crawler_config.py
# ä¿®æ”¹: GITHUB_TOKEN = "ghp_ä½ çš„token"

# æ–¹å¼2: åˆ›å»ºæœ¬åœ°é…ç½®
cat > crawler_config_local.py << 'EOF'
GITHUB_TOKEN = "ghp_ä½ çš„token"
EOF
```

**å¥½å¤„**ï¼š
- æ— Token: 60è¯·æ±‚/å°æ—¶ âš ï¸
- æœ‰Token: 5000è¯·æ±‚/å°æ—¶ âœ…

### æ­¥éª¤3ï¼šå¼€å§‹çˆ¬å–

```bash
# æ–¹å¼1: ä½¿ç”¨æ”¹è¿›åçš„çˆ¬è™«ï¼ˆæ¨èï¼‰
python advanced_crawler.py

# æ–¹å¼2: æ¼”ç¤ºæ¨¡å¼ï¼ˆå°‘é‡æ•°æ®æµ‹è¯•ï¼‰
python demo.py

# æ–¹å¼3: åªæ¸…ç†ä¸çˆ¬å–
python clean_bugs.py bugs.json
```

---

## ğŸ“Š æ•°æ®è´¨é‡å¯¹æ¯”

### æ¸…ç†å‰ âŒ

```json
{
    "test": [
        "CREATE TABLE test (...);",
        "[481c0ddc8f6c] 2025.10.31 10:39:53.190487 [ 88 ] <Fatal>...",
        "Stack trace: 0x00007038755da9fd 0x0000703875586476...",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“",
        "â”ƒ statement                                        â”ƒ",
        "Query id: 8e91dc37-ab10-48bf-8c30-4b73ba2d49d5",
        "(false,54,379.64,'Arn','2021-11-20'),(true,760,...",
        ...
    ]
}
```

### æ¸…ç†å âœ…

```json
{
    "test": [
        "CREATE TABLE compression_estimate_example (",
        "ORDER BY number;",
        "SELECT number FROM system.numbers LIMIT 100_000;",
        "SELECT estimateCompressionRatio('DoubleDelta, T64, ZSTD')(number) AS estimate FROM compression_estimate_example;"
    ]
}
```

---

## ğŸ¯ å¸¸ç”¨å‘½ä»¤

### æŸ¥çœ‹ç»Ÿè®¡

```bash
# åŸå§‹æ•°æ®ç»Ÿè®¡
python stats.py

# æŸ¥çœ‹æœ€æ–°bug
tail -20 bugs.json

# ç»Ÿè®¡bugæ•°é‡
python -c "import json; print(f'æ€»bugæ•°: {len(json.load(open(\"bugs.json\")))}')"
```

### çˆ¬å–æ“ä½œ

```bash
# å®Œæ•´çˆ¬å–ï¼ˆDuckDB + ClickHouse + PostgreSQLï¼‰
python advanced_crawler.py

# åªçˆ¬å–DuckDBï¼ˆä¿®æ”¹advanced_crawler.pyçš„mainå‡½æ•°ï¼‰
# æ³¨é‡Šæ‰å…¶ä»–æ•°æ®åº“çš„çˆ¬å–ä»£ç 

# æ¼”ç¤ºæ¨¡å¼ï¼ˆå°‘é‡æ•°æ®ï¼‰
python demo.py
```

### æ¸…ç†æ“ä½œ

```bash
# æ¸…ç†bugs.json
python clean_bugs.py bugs.json

# æ¸…ç†å¹¶æŸ¥çœ‹ç»Ÿè®¡
python clean_bugs.py bugs.json && python stats.py

# æ¢å¤å¤‡ä»½
cp bugs.json.backup.dirty bugs.json
```

---

## ğŸ”§ è‡ªå®šä¹‰çˆ¬å–

### ä¿®æ”¹çˆ¬å–å‚æ•°

ç¼–è¾‘ `advanced_crawler.py` çš„ `main()` å‡½æ•°ï¼š

```python
# å¢åŠ çˆ¬å–æ•°é‡
duckdb_count = crawler.crawl_repo(
    repo="duckdb/duckdb",
    dbms_name="DuckDB",
    max_issues=500,  # é»˜è®¤200
    since_date="2020-01-01",  # ä¿®æ”¹èµ·å§‹æ—¥æœŸ
    labels=["bug"]
)

# åªçˆ¬å–crashç±»å‹
labels=["bug", "crash"]

# åªçˆ¬å–æœªå…³é—­çš„bug
# åœ¨crawl_repoæ–¹æ³•ä¸­ä¿®æ”¹params['state'] = 'open'
```

### æ·»åŠ æ–°æ•°æ®åº“

åœ¨ `main()` å‡½æ•°ä¸­æ·»åŠ ï¼š

```python
# MySQL
mysql_count = crawler.crawl_repo(
    repo="mysql/mysql-server",
    dbms_name="MySQL",
    max_issues=100,
    since_date="2020-01-01",
    labels=["bug"]
)
```

---

## ğŸ“ˆ æ¸…ç†è„šæœ¬ç‰¹æ€§

### è‡ªåŠ¨è¿‡æ»¤çš„å†…å®¹

âœ… **æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯**
- `[timestamp]` æ ¼å¼çš„æ—¥å¿—
- `2025.10.31 10:39:53` æ—¶é—´æˆ³
- `<Fatal>`, `<Error>` æ—¥å¿—çº§åˆ«
- `Stack trace:` å †æ ˆè·Ÿè¸ª

âœ… **è¡¨æ ¼å’Œæ ¼å¼åŒ–å†…å®¹**
- `â”â”â”â”“` è¡¨æ ¼è¾¹æ¡†
- `â”ƒ ... â”ƒ` è¡¨æ ¼å†…å®¹
- `â”‚â†³` è¡¨æ ¼æ ‡è®°
- `Query id:` æŸ¥è¯¢ID

âœ… **æ•°æ®å†…å®¹**
- `(val1, val2, val3, ...)` å¤§é‡æ•°æ®å…ƒç»„
- è¶…è¿‡10ä¸ªé€—å·çš„éSQLè¡Œ
- åå…­è¿›åˆ¶åœ°å€ `0x...`

âœ… **GitHubæ¨¡æ¿**
- `### Company or project name`
- å…¶ä»–issueæ¨¡æ¿æ–‡æœ¬

### ä¿ç•™çš„å†…å®¹

âœ… **çº¯å‡€SQLè¯­å¥**
```sql
CREATE TABLE test (...);
SELECT * FROM test WHERE ...;
INSERT INTO test VALUES (...);
```

âœ… **æœ‰ç”¨çš„æ³¨é‡Š**
- ç®€çŸ­çš„bugæè¿°ï¼ˆ<300å­—ç¬¦ï¼‰
- æŠ€æœ¯ç»†èŠ‚è¯´æ˜

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1ï¼šçˆ¬å–åˆ°çš„SQLä»ç„¶æœ‰å™ªéŸ³

**è§£å†³æ–¹æ¡ˆ1** - è°ƒæ•´è¿‡æ»¤è§„åˆ™ï¼š

ç¼–è¾‘ `advanced_crawler.py`ï¼Œåœ¨ `_is_noise_line()` å‡½æ•°ä¸­æ·»åŠ æ›´å¤šæ¨¡å¼ï¼š

```python
noise_patterns = [
    # ... ç°æœ‰è§„åˆ™ ...
    r'^ä½ çš„æ–°è§„åˆ™',  # æ·»åŠ è¿™é‡Œ
]
```

**è§£å†³æ–¹æ¡ˆ2** - æ‰‹åŠ¨æ¸…ç†ï¼š

```bash
python clean_bugs.py bugs.json
```

### é—®é¢˜2ï¼šæ¸…ç†åä¸¢å¤±äº†æŸäº›SQL

**åŸå› **ï¼šSQLè¯­å¥è¢«è¯¯åˆ¤ä¸ºå™ªéŸ³

**è§£å†³æ–¹æ¡ˆ**ï¼š

ç¼–è¾‘ `clean_bugs.py`ï¼Œè°ƒæ•´ `looks_like_sql()` å‡½æ•°ï¼š

```python
def looks_like_sql(line: str) -> bool:
    # é™ä½åˆ¤æ–­æ¡ä»¶
    sql_keywords = ['SELECT', 'CREATE', 'INSERT', ...]
    return any(kw in line.upper() for kw in sql_keywords)
```

### é—®é¢˜3ï¼šAPIé™æµ

```
â° APIé™æµï¼Œç­‰å¾… 3600ç§’...
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ç­‰å¾…1å°æ—¶
2. è®¾ç½®GitHub Tokenï¼ˆæå‡åˆ°5000è¯·æ±‚/å°æ—¶ï¼‰
3. å‡å°‘max_issueså‚æ•°

### é—®é¢˜4ï¼šçˆ¬å–åˆ°0ä¸ªæ–°bug

**åŸå› **ï¼š
- æ‰€æœ‰bugéƒ½å·²å­˜åœ¨ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
- `since_date` è®¾ç½®è¿‡æ™š
- æ ‡ç­¾è¿‡æ»¤å¤ªä¸¥æ ¼

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# è°ƒæ•´since_date
since_date="2020-01-01"  # æ›´æ—©çš„æ—¥æœŸ

# æ”¾å®½æ ‡ç­¾
labels=[]  # ä¸è¿‡æ»¤æ ‡ç­¾
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. å®šæœŸçˆ¬å–

```bash
# æ¯å‘¨è¿è¡Œä¸€æ¬¡
cd /root/QTRAN/mydata/bugjson
python advanced_crawler.py
python clean_bugs.py bugs.json
```

### 2. å¢é‡çˆ¬å–

```python
# è®¾ç½®since_dateä¸ºä¸Šæ¬¡çˆ¬å–æ—¥æœŸ
since_date="2025-10-31"  # bugs.jsonä¸­æœ€æ–°çš„æ—¥æœŸ
```

### 3. æ•°æ®å¤‡ä»½

```bash
# å®šæœŸå¤‡ä»½
cp bugs.json bugs.json.backup.$(date +%Y%m%d)
```

### 4. è´¨é‡æ£€æŸ¥

```bash
# æ£€æŸ¥æœ€æ–°10ä¸ªbug
python -c "
import json
bugs = json.load(open('bugs.json'))
for b in bugs[-10:]:
    print(f'{b[\"date\"]} - {b[\"dbms\"]} - {b[\"title\"][:60]}')
    if b.get('test'):
        print(f'  âœ… SQL: {len(b[\"test\"])}æ¡')
    else:
        print(f'  âš ï¸  æ— SQL')
"
```

---

## ğŸ“ è·å–å¸®åŠ©

### æ–‡æ¡£

- `README.md` - è¯¦ç»†æ–‡æ¡£
- `QUICKSTART.md` - å¿«é€Ÿå¼€å§‹
- `æœ¬æ–‡ä»¶` - ä½¿ç”¨æŒ‡å—

### æ£€æŸ¥å·¥å…·

```bash
# æŸ¥çœ‹çˆ¬è™«åŠŸèƒ½
python advanced_crawler.py --help

# æŸ¥çœ‹ç»Ÿè®¡
python stats.py

# æµ‹è¯•æ¸…ç†
python clean_bugs.py bugs_demo.json
```

---

## âœ… æ€»ç»“

### å·²åˆ›å»ºçš„å·¥å…·

1. âœ… `advanced_crawler.py` - æ”¹è¿›åçš„çˆ¬è™«ï¼ˆè¿‡æ»¤å™ªéŸ³ï¼‰
2. âœ… `clean_bugs.py` - æ•°æ®æ¸…ç†å·¥å…·
3. âœ… `stats.py` - ç»Ÿè®¡åˆ†æå·¥å…·
4. âœ… `demo.py` - æ¼”ç¤ºè„šæœ¬

### æ•°æ®è´¨é‡ä¿è¯

1. âœ… è‡ªåŠ¨è¿‡æ»¤æ—¥å¿—å’Œå †æ ˆè·Ÿè¸ª
2. âœ… åªä¿ç•™çº¯å‡€SQLè¯­å¥
3. âœ… ç§»é™¤GitHubæ¨¡æ¿æ–‡æœ¬
4. âœ… è‡ªåŠ¨å¤‡ä»½åŸå§‹æ•°æ®

### æ¨èå·¥ä½œæµç¨‹

```bash
# 1. é…ç½®Token
vim crawler_config.py

# 2. è¿è¡Œçˆ¬è™«
python advanced_crawler.py

# 3. æ¸…ç†æ•°æ®
python clean_bugs.py bugs.json

# 4. æŸ¥çœ‹ç»Ÿè®¡
python stats.py

# 5. æ£€æŸ¥è´¨é‡
tail -50 bugs.json
```

---

**æœ€åæ›´æ–°**: 2025-10-31  
**ç‰ˆæœ¬**: v2.0ï¼ˆå·²ä¼˜åŒ–ï¼‰

