# 数据库Bug爬虫 - 完整使用指南

## 📋 问题解决

### ✅ 已解决：爬取数据"乱七八糟"的问题

**原因**：
- 从GitHub issues中提取的内容包含日志、堆栈跟踪、表格边框等噪音
- SQL语句混杂在错误信息中

**解决方案**：
1. ✅ 改进了SQL提取算法（`advanced_crawler.py`）
2. ✅ 添加了噪音过滤逻辑（过滤日志、堆栈跟踪、表格边框等）
3. ✅ 创建了数据清理工具（`clean_bugs.py`）

---

## 🚀 三步快速开始

### 步骤1：清理已有数据（如果需要）

```bash
cd /root/QTRAN/mydata/bugjson

# 清理bugs.json（会自动备份）
python clean_bugs.py bugs.json

# 清理演示数据
python clean_bugs.py bugs_demo.json
```

**效果**：
- ✅ 自动备份原文件到 `bugs.json.backup.dirty`
- ✅ 移除test字段中的日志、错误信息
- ✅ 只保留纯净的SQL语句
- ✅ 移除无关的comment内容

### 步骤2：配置GitHub Token（推荐）

```bash
# 获取Token: https://github.com/settings/tokens
# 需要权限: public_repo

# 方式1: 直接编辑配置文件
vim crawler_config.py
# 修改: GITHUB_TOKEN = "ghp_你的token"

# 方式2: 创建本地配置
cat > crawler_config_local.py << 'EOF'
GITHUB_TOKEN = "ghp_你的token"
EOF
```

**好处**：
- 无Token: 60请求/小时 ⚠️
- 有Token: 5000请求/小时 ✅

### 步骤3：开始爬取

```bash
# 方式1: 使用改进后的爬虫（推荐）
python advanced_crawler.py

# 方式2: 演示模式（少量数据测试）
python demo.py

# 方式3: 只清理不爬取
python clean_bugs.py bugs.json
```

---

## 📊 数据质量对比

### 清理前 ❌

```json
{
    "test": [
        "CREATE TABLE test (...);",
        "[481c0ddc8f6c] 2025.10.31 10:39:53.190487 [ 88 ] <Fatal>...",
        "Stack trace: 0x00007038755da9fd 0x0000703875586476...",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓",
        "┃ statement                                        ┃",
        "Query id: 8e91dc37-ab10-48bf-8c30-4b73ba2d49d5",
        "(false,54,379.64,'Arn','2021-11-20'),(true,760,...",
        ...
    ]
}
```

### 清理后 ✅

```json
{
    "test": [
        "CREATE TABLE compression_estimate_example (",
        "ORDER BY number;",
        "SELECT number FROM system.numbers LIMIT 100_000;",
        "SELECT estimateCompressionRatio('DoubleDelta, T64, ZSTD')(number) AS estimate FROM compression_estimate_example;"
    ]
}
```

---

## 🎯 常用命令

### 查看统计

```bash
# 原始数据统计
python stats.py

# 查看最新bug
tail -20 bugs.json

# 统计bug数量
python -c "import json; print(f'总bug数: {len(json.load(open(\"bugs.json\")))}')"
```

### 爬取操作

```bash
# 完整爬取（DuckDB + ClickHouse + PostgreSQL）
python advanced_crawler.py

# 只爬取DuckDB（修改advanced_crawler.py的main函数）
# 注释掉其他数据库的爬取代码

# 演示模式（少量数据）
python demo.py
```

### 清理操作

```bash
# 清理bugs.json
python clean_bugs.py bugs.json

# 清理并查看统计
python clean_bugs.py bugs.json && python stats.py

# 恢复备份
cp bugs.json.backup.dirty bugs.json
```

---

## 🔧 自定义爬取

### 修改爬取参数

编辑 `advanced_crawler.py` 的 `main()` 函数：

```python
# 增加爬取数量
duckdb_count = crawler.crawl_repo(
    repo="duckdb/duckdb",
    dbms_name="DuckDB",
    max_issues=500,  # 默认200
    since_date="2020-01-01",  # 修改起始日期
    labels=["bug"]
)

# 只爬取crash类型
labels=["bug", "crash"]

# 只爬取未关闭的bug
# 在crawl_repo方法中修改params['state'] = 'open'
```

### 添加新数据库

在 `main()` 函数中添加：

```python
# MySQL
mysql_count = crawler.crawl_repo(
    repo="mysql/mysql-server",
    dbms_name="MySQL",
    max_issues=100,
    since_date="2020-01-01",
    labels=["bug"]
)
```

---

## 📈 清理脚本特性

### 自动过滤的内容

✅ **日志和错误信息**
- `[timestamp]` 格式的日志
- `2025.10.31 10:39:53` 时间戳
- `<Fatal>`, `<Error>` 日志级别
- `Stack trace:` 堆栈跟踪

✅ **表格和格式化内容**
- `┏━━┓` 表格边框
- `┃ ... ┃` 表格内容
- `│↳` 表格标记
- `Query id:` 查询ID

✅ **数据内容**
- `(val1, val2, val3, ...)` 大量数据元组
- 超过10个逗号的非SQL行
- 十六进制地址 `0x...`

✅ **GitHub模板**
- `### Company or project name`
- 其他issue模板文本

### 保留的内容

✅ **纯净SQL语句**
```sql
CREATE TABLE test (...);
SELECT * FROM test WHERE ...;
INSERT INTO test VALUES (...);
```

✅ **有用的注释**
- 简短的bug描述（<300字符）
- 技术细节说明

---

## 🐛 故障排查

### 问题1：爬取到的SQL仍然有噪音

**解决方案1** - 调整过滤规则：

编辑 `advanced_crawler.py`，在 `_is_noise_line()` 函数中添加更多模式：

```python
noise_patterns = [
    # ... 现有规则 ...
    r'^你的新规则',  # 添加这里
]
```

**解决方案2** - 手动清理：

```bash
python clean_bugs.py bugs.json
```

### 问题2：清理后丢失了某些SQL

**原因**：SQL语句被误判为噪音

**解决方案**：

编辑 `clean_bugs.py`，调整 `looks_like_sql()` 函数：

```python
def looks_like_sql(line: str) -> bool:
    # 降低判断条件
    sql_keywords = ['SELECT', 'CREATE', 'INSERT', ...]
    return any(kw in line.upper() for kw in sql_keywords)
```

### 问题3：API限流

```
⏰ API限流，等待 3600秒...
```

**解决方案**：
1. 等待1小时
2. 设置GitHub Token（提升到5000请求/小时）
3. 减少max_issues参数

### 问题4：爬取到0个新bug

**原因**：
- 所有bug都已存在（自动去重）
- `since_date` 设置过晚
- 标签过滤太严格

**解决方案**：

```python
# 调整since_date
since_date="2020-01-01"  # 更早的日期

# 放宽标签
labels=[]  # 不过滤标签
```

---

## 📝 最佳实践

### 1. 定期爬取

```bash
# 每周运行一次
cd /root/QTRAN/mydata/bugjson
python advanced_crawler.py
python clean_bugs.py bugs.json
```

### 2. 增量爬取

```python
# 设置since_date为上次爬取日期
since_date="2025-10-31"  # bugs.json中最新的日期
```

### 3. 数据备份

```bash
# 定期备份
cp bugs.json bugs.json.backup.$(date +%Y%m%d)
```

### 4. 质量检查

```bash
# 检查最新10个bug
python -c "
import json
bugs = json.load(open('bugs.json'))
for b in bugs[-10:]:
    print(f'{b[\"date\"]} - {b[\"dbms\"]} - {b[\"title\"][:60]}')
    if b.get('test'):
        print(f'  ✅ SQL: {len(b[\"test\"])}条')
    else:
        print(f'  ⚠️  无SQL')
"
```

---

## 📞 获取帮助

### 文档

- `README.md` - 详细文档
- `QUICKSTART.md` - 快速开始
- `本文件` - 使用指南

### 检查工具

```bash
# 查看爬虫功能
python advanced_crawler.py --help

# 查看统计
python stats.py

# 测试清理
python clean_bugs.py bugs_demo.json
```

---

## ✅ 总结

### 已创建的工具

1. ✅ `advanced_crawler.py` - 改进后的爬虫（过滤噪音）
2. ✅ `clean_bugs.py` - 数据清理工具
3. ✅ `stats.py` - 统计分析工具
4. ✅ `demo.py` - 演示脚本

### 数据质量保证

1. ✅ 自动过滤日志和堆栈跟踪
2. ✅ 只保留纯净SQL语句
3. ✅ 移除GitHub模板文本
4. ✅ 自动备份原始数据

### 推荐工作流程

```bash
# 1. 配置Token
vim crawler_config.py

# 2. 运行爬虫
python advanced_crawler.py

# 3. 清理数据
python clean_bugs.py bugs.json

# 4. 查看统计
python stats.py

# 5. 检查质量
tail -50 bugs.json
```

---

**最后更新**: 2025-10-31  
**版本**: v2.0（已优化）

