# 多数据库Bug爬取说明

## ✅ 已实现的数据库爬虫

你现在可以同时爬取以下数据库的bugs：

| 数据库 | GitHub仓库 | 状态 | 说明 |
|--------|------------|------|------|
| **DuckDB** | `duckdb/duckdb` | ✅ 活跃 | 大量bug issues |
| **ClickHouse** | `ClickHouse/ClickHouse` | ✅ 活跃 | 大量bug issues |
| **MonetDB** | `MonetDB/MonetDB` | ✅ 活跃 | 大量bug issues |
| **PostgreSQL** | `postgres/postgres` | ⚠️ 镜像 | 官方仓库镜像，issues较少 |
| **MySQL** | `mysql/mysql-server` | ⚠️ 镜像 | 官方仓库镜像，issues较少 |
| **MariaDB** | `MariaDB/server` | ⚠️ 少量 | issues数量较少 |
| **SQLite** | - | ❌ 不适用 | 使用自己的bug tracker |

---

## 🚀 使用方法

### 方式1：使用多数据库爬虫（推荐）

```bash
cd /root/QTRAN/mydata/bugjson

# 每个数据库爬取20个bug
python crawl_multi_db.py 20

# 每个数据库爬取50个bug
python crawl_multi_db.py 50

# 每个数据库爬取100个bug
python crawl_multi_db.py 100
```

**输出文件**: `bugs_multi_db.json`

### 方式2：使用advanced_crawler（完整爬取）

```bash
# 爬取所有配置的数据库
python advanced_crawler.py
```

**输出文件**: `bugs.json`

### 方式3：使用demo（快速测试）

```bash
# 每个数据库爬取10个bug用于演示
python demo.py
```

**输出文件**: `bugs_demo.json`

---

## 📊 爬取结果示例

### 实际爬取测试结果

```bash
$ python crawl_multi_db.py 20
```

```
📊 爬取完成统计:
══════════════════════════════════════
  ⚠️  DuckDB            0 个新bug
  ✅ ClickHouse      100 个新bug
  ⚠️  PostgreSQL        0 个新bug
  ⚠️  MySQL             0 个新bug
  ✅ MonetDB         100 个新bug
  ⚠️  MariaDB           0 个新bug
  ────────────────────────────────────
  总计:          200 个新bug
  总bug数:       200 个
══════════════════════════════════════

📊 数据库分布:
  ClickHouse       100 ( 50.0%)
  MonetDB          100 ( 50.0%)
```

### 为什么某些数据库爬取不到？

**1. DuckDB (0个)**
- 原因：bugs.json中已经有DuckDB的bugs，自动去重
- 解决：调整`since_date`参数到更晚的日期

**2. PostgreSQL (0个)**
- 原因：GitHub上的postgres/postgres是只读镜像
- 官方bug tracker：https://www.postgresql.org/support/submitbug/
- 解决：需要从官方邮件列表爬取

**3. MySQL (0个)**
- 原因：GitHub上的mysql/mysql-server issues较少
- 官方bug tracker：https://bugs.mysql.com/
- 解决：需要实现Bugzilla API爬虫

**4. MariaDB (0个)**
- 原因：GitHub仓库issues数量很少（只有8个）
- 官方bug tracker：https://jira.mariadb.org/
- 解决：需要实现Jira API爬虫

**5. SQLite (不支持)**
- 原因：SQLite不使用GitHub
- 官方bug tracker：https://www.sqlite.org/src/reportlist
- 解决：需要实现专门的SQLite爬虫

---

## 🔧 自定义配置

### 调整爬取参数

编辑 `crawl_multi_db.py`:

```python
# 修改起始日期（获取更旧的bug）
since_date="2019-01-01"  # 默认 2020-01-01

# 修改每个数据库的数量
databases = [
    {"repo": "duckdb/duckdb", "name": "DuckDB", 
     "max_issues": 500, "labels": ["bug"]},  # 改为500
    ...
]
```

### 添加新数据库

在 `databases` 列表中添加：

```python
{
    "repo": "owner/repo",
    "name": "DatabaseName",
    "labels": ["bug"]
}
```

---

## 💡 提高爬取成功率的技巧

### 1. 使用GitHub Token

```bash
vim crawler_config.py
# 添加: GITHUB_TOKEN = "ghp_你的token"
```

**好处**：
- 无Token: 60请求/小时
- 有Token: 5000请求/小时

### 2. 调整since_date

```python
# 爬取更久远的数据
since_date="2015-01-01"
```

### 3. 移除标签过滤

```python
# 不过滤标签，获取所有issues
labels=[]
```

### 4. 增加max_issues

```python
# 检查更多issues
max_issues=500
```

---

## 📈 获取更多数据库bugs的方法

### 方法1：从bugs.json中筛选

```bash
# 查看bugs.json中有哪些数据库
python stats.py

# 筛选特定数据库
python -c "
import json
bugs = json.load(open('bugs.json'))
sqlite_bugs = [b for b in bugs if b['dbms'] == 'SQLite']
print(f'SQLite bugs: {len(sqlite_bugs)}')
"
```

### 方法2：使用不同的数据源

**DuckDB**: 
- GitHub issues活跃 ✅
- 可以爬取大量数据

**SQLite**:
- 从bugs.json中已有193个SQLite bugs
- 官方tracker: https://www.sqlite.org/src/reportlist

**MySQL**:
- Bugzilla: https://bugs.mysql.com/
- 需要实现专门的爬虫

**PostgreSQL**:
- 邮件列表: https://www.postgresql.org/list/pgsql-bugs/
- 需要解析邮件格式

---

## 🎯 推荐工作流程

### 1. 快速测试（获取少量数据）

```bash
# 每个数据库10-20个bug
python crawl_multi_db.py 20
```

### 2. 完整爬取（获取大量数据）

```bash
# 每个数据库100-200个bug
python crawl_multi_db.py 200
```

### 3. 清理数据

```bash
# 移除噪音
python clean_bugs.py bugs_multi_db.json
```

### 4. 合并到主文件

```bash
python -c "
import json
import shutil

# 备份
shutil.copy('bugs.json', 'bugs.json.backup')

# 加载
main_bugs = json.load(open('bugs.json'))
new_bugs = json.load(open('bugs_multi_db.json'))

# 去重合并
existing_links = set()
for bug in main_bugs:
    for link in bug.get('links', {}).values():
        existing_links.add(link)

added = 0
for bug in new_bugs:
    bug_link = bug.get('links', {}).get('bugreport', '')
    if bug_link and bug_link not in existing_links:
        main_bugs.append(bug)
        existing_links.add(bug_link)
        added += 1

# 保存
json.dump(main_bugs, open('bugs.json', 'w'), indent=4, ensure_ascii=False)
print(f'✅ 新增 {added} 个bug，总计 {len(main_bugs)} 个')
"
```

---

## 📊 当前数据集状态

从 `bugs.json` 中已有的数据：

```
总bug数: 699个
数据库分布:
  ✅ ClickHouse:  200个 (28.6%)
  ✅ SQLite:      193个 (27.6%)
  ✅ DuckDB:       75个 (10.7%)
  ✅ CockroachDB:  68个 (9.7%)
  ✅ TiDB:         62个 (8.9%)
  ✅ MySQL:        40个 (5.7%)
  ✅ PostgreSQL:   31个 (4.4%)
  ✅ 其他:        30个
```

**说明**：
- ✅ bugs.json中已经包含多种数据库
- ✅ SQLite有193个（无需额外爬取）
- ✅ MySQL有40个（无需额外爬取）
- ✅ PostgreSQL有31个（无需额外爬取）

---

## ✅ 总结

### 现在支持的功能

1. ✅ **同时爬取6个数据库**
   - DuckDB、ClickHouse、PostgreSQL
   - MySQL、MonetDB、MariaDB

2. ✅ **自动去重**
   - 检查已有链接
   - 避免重复数据

3. ✅ **智能过滤**
   - 移除日志和堆栈跟踪
   - 只保留纯净SQL

4. ✅ **详细统计**
   - 每个数据库的爬取结果
   - 数据库分布图

### 使用建议

```bash
# 1. 快速获取多数据库样本
python crawl_multi_db.py 20

# 2. 清理数据
python clean_bugs.py bugs_multi_db.json

# 3. 查看结果
python -c "import json; bugs=json.load(open('bugs_multi_db.json')); \
  from collections import Counter; print(Counter(b['dbms'] for b in bugs))"

# 4. 如果想要更多SQLite/MySQL/PostgreSQL的bugs
#    直接使用bugs.json，里面已经有了！
python stats.py  # 查看bugs.json的统计
```

---

**最后更新**: 2025-10-31  
**版本**: v2.0

