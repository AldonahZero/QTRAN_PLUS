#!/usr/bin/env python3
"""
çˆ¬è™«æ¼”ç¤ºè„šæœ¬ - çˆ¬å–å°‘é‡æ•°æ®è¿›è¡Œæµ‹è¯•
"""

from advanced_crawler import GitHubIssueCrawler
import json


def demo_crawl():
    """æ¼”ç¤ºçˆ¬è™«åŠŸèƒ½"""
    
    print("=" * 70)
    print("ğŸ¬ æ•°æ®åº“Bugçˆ¬è™«æ¼”ç¤º")
    print("=" * 70)
    print("\nè¿™ä¸ªæ¼”ç¤ºå°†çˆ¬å–å°‘é‡bugæ•°æ®ï¼Œç”¨äºæµ‹è¯•çˆ¬è™«åŠŸèƒ½\n")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä¸ä½¿ç”¨tokenï¼Œä»…æ¼”ç¤ºï¼‰
    print("1ï¸âƒ£  åˆ›å»ºçˆ¬è™«å®ä¾‹...")
    crawler = GitHubIssueCrawler(output_file="bugs_demo.json")
    
    # ä¿å­˜åŸå§‹bugæ•°é‡
    original_count = len(crawler.existing_bugs)
    print(f"   åŸå§‹bugæ•°é‡: {original_count}")
    
    # å®šä¹‰è¦çˆ¬å–çš„æ•°æ®åº“
    databases = [
        ("duckdb/duckdb", "DuckDB", ["bug"]),
        ("ClickHouse/ClickHouse", "ClickHouse", ["bug"]),
        ("postgres/postgres", "PostgreSQL", []),
        ("mysql/mysql-server", "MySQL", ["Bug"]),
        ("MonetDB/MonetDB", "MonetDB", ["bug"]),
        ("MariaDB/server", "MariaDB", ["bug"]),
    ]
    
    counts = {}
    
    # çˆ¬å–æ¯ä¸ªæ•°æ®åº“
    for i, (repo, dbms_name, labels) in enumerate(databases, 2):
        print(f"\n{i}ï¸âƒ£  çˆ¬å–{dbms_name}æœ€æ–°çš„bug...")
        count = crawler.crawl_repo(
            repo=repo,
            dbms_name=dbms_name,
            max_issues=10,  # æ¯ä¸ªæ•°æ®åº“10ä¸ªç”¨äºæ¼”ç¤º
            labels=labels
        )
        counts[dbms_name] = count
    
    duckdb_count = counts.get("DuckDB", 0)
    clickhouse_count = counts.get("ClickHouse", 0)
    
    # è®¡ç®—æ€»æ–°å¢
    total_new = sum(counts.values())
    
    # ä¿å­˜ç»“æœ
    if total_new > 0:
        print(f"\n{len(databases)+1}ï¸âƒ£  ä¿å­˜ç»“æœ...")
        crawler.save_bugs(backup=False)
        
        # æ˜¾ç¤ºæ–°å¢çš„bug
        print("\n5ï¸âƒ£  æ–°å¢çš„bugç¤ºä¾‹:")
        print("   " + "â”€" * 66)
        new_bugs = crawler.existing_bugs[original_count:original_count + 3]
        for i, bug in enumerate(new_bugs, 1):
            print(f"\n   Bug #{i}:")
            print(f"   ğŸ“Œ æ ‡é¢˜: {bug['title'][:55]}...")
            print(f"   ğŸ—“ï¸  æ—¥æœŸ: {bug['date']}")
            print(f"   ğŸ’¾ æ•°æ®åº“: {bug['dbms']}")
            print(f"   ğŸ” Oracle: {bug['oracle']}")
            print(f"   ğŸ‘¤ Reporter: {bug['reporter']}")
            print(f"   ğŸ“Š çŠ¶æ€: {bug['status']}")
            if bug.get('test'):
                print(f"   ğŸ§ª SQLæµ‹è¯•: {len(bug['test'])}æ¡")
                if bug['test']:
                    print(f"      - {bug['test'][0][:50]}...")
    
    # ç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤ºç»Ÿè®¡:")
    print("=" * 70)
    for dbms_name, count in counts.items():
        status = "âœ…" if count > 0 else "âš ï¸ "
        print(f"  {status} {dbms_name:15s} {count:3d} ä¸ª")
    print(f"  {'â”€' * 36}")
    print(f"  æ€»æ–°å¢:         {total_new:3d} ä¸ª")
    print(f"  å½“å‰æ€»æ•°:       {len(crawler.existing_bugs):3d} ä¸ª")
    print("=" * 70)
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆ!")
    print("\nğŸ’¡ æç¤º:")
    print("   - æ¼”ç¤ºç»“æœä¿å­˜åœ¨: bugs_demo.json")
    print("   - å®Œæ•´çˆ¬å–ä½¿ç”¨: python advanced_crawler.py")
    print("   - æŸ¥çœ‹ç»Ÿè®¡: python stats.py")
    print("   - è¯¦ç»†æ–‡æ¡£: cat README.md")
    

if __name__ == "__main__":
    demo_crawl()

