#!/usr/bin/env python3
"""
å¤šæºBugçˆ¬è™«æ¶æ„ - ä¸ºä¸åŒæ•°æ®åº“ä½¿ç”¨ä¸åŒçš„çˆ¬å–æ–¹æ³•

æ”¯æŒçš„æ•°æ®æºï¼š
1. SQLite - å®˜æ–¹bug tracker (HTMLè§£æ)
2. DuckDB - GitHub Issues (æ”¾å®½è¿‡æ»¤)
3. MySQL - Bugzilla API
4. PostgreSQL - å®˜æ–¹é‚®ä»¶åˆ—è¡¨/bug tracker
5. MariaDB - Jira API
6. MonetDB - GitHub Issues
7. ClickHouse - GitHub Issues
"""

import requests
import re
import json
import time
from datetime import datetime
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from abc import ABC, abstractmethod


class BaseBugCrawler(ABC):
    """Bugçˆ¬è™«åŸºç±»"""
    
    def __init__(self, dbms_name: str):
        self.dbms_name = dbms_name
        self.bugs = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        })
    
    @abstractmethod
    def crawl(self, max_bugs: int = 100) -> List[Dict]:
        """çˆ¬å–bugs - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def save_bugs(self, filename: str):
        """ä¿å­˜bugsåˆ°JSONæ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.bugs, f, indent=4, ensure_ascii=False)
        print(f"  ğŸ’¾ {self.dbms_name}: ä¿å­˜ {len(self.bugs)} ä¸ªbugsåˆ° {filename}")


class SQLiteBugCrawler(BaseBugCrawler):
    """SQLite Bug Trackerçˆ¬è™« - HTMLè§£æ"""
    
    def __init__(self):
        super().__init__("SQLite")
        self.base_url = "https://www.sqlite.org/src/timeline"
    
    def crawl(self, max_bugs: int = 100) -> List[Dict]:
        """
        çˆ¬å–SQLiteçš„bugæŠ¥å‘Š
        
        SQLiteä½¿ç”¨Fossilç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œæœ‰ä¸“é—¨çš„timelineé¡µé¢
        """
        print(f"\nğŸ” çˆ¬å– {self.dbms_name} (å®˜æ–¹Bug Tracker)...")
        
        try:
            # SQLiteçš„timelineå¯ä»¥æŒ‰ç±»å‹è¿‡æ»¤
            # å‚æ•°: y=ci (check-ins), n=100 (æ•°é‡)
            url = f"{self.base_url}?y=ci&n={max_bugs*2}"  # å¤šçˆ¬ä¸€äº›ï¼Œè¿‡æ»¤åå¯èƒ½ä¸å¤Ÿ
            
            print(f"  ğŸ“„ è®¿é—®: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾åŒ…å«"fix"æˆ–"bug"å…³é”®è¯çš„æäº¤
            timeline_items = soup.find_all('tr', class_='timelineRow')
            
            count = 0
            for item in timeline_items:
                if count >= max_bugs:
                    break
                
                # æå–ä¿¡æ¯
                bug_data = self._parse_timeline_item(item)
                if bug_data:
                    self.bugs.append(bug_data)
                    count += 1
                    print(f"    âœ… {bug_data['title'][:60]}...")
            
            print(f"  ğŸ‰ {self.dbms_name}: æ–°å¢ {len(self.bugs)} ä¸ªbugs")
            
        except Exception as e:
            print(f"  âŒ çˆ¬å–å¤±è´¥: {e}")
        
        return self.bugs
    
    def _parse_timeline_item(self, item) -> Optional[Dict]:
        """è§£ætimelineæ¡ç›®"""
        try:
            # æŸ¥æ‰¾æ˜¯å¦åŒ…å«bugç›¸å…³å…³é”®è¯
            text = item.get_text().lower()
            if not any(kw in text for kw in ['fix', 'bug', 'crash', 'error', 'issue']):
                return None
            
            # æå–æ—¥æœŸ
            date_elem = item.find('td', class_='timelineDate')
            date_str = date_elem.get_text().strip() if date_elem else ''
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = item.find('a')
            title = title_elem.get_text().strip() if title_elem else 'No title'
            link = f"https://www.sqlite.org{title_elem['href']}" if title_elem and title_elem.get('href') else ''
            
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            try:
                # SQLiteä½¿ç”¨ YYYY-MM-DD æ ¼å¼
                date_obj = datetime.strptime(date_str.split()[0], '%Y-%m-%d')
                formatted_date = date_obj.strftime('%d/%m/%Y')
            except:
                formatted_date = date_str
            
            return {
                "date": formatted_date,
                "dbms": self.dbms_name,
                "links": {
                    "bugreport": link
                },
                "oracle": "unknown",
                "reporter": "SQLite Team",
                "status": "fixed",
                "title": title
            }
            
        except Exception as e:
            return None


class DuckDBGitHubCrawler(BaseBugCrawler):
    """DuckDB GitHubçˆ¬è™« - ä¸ä½¿ç”¨æ ‡ç­¾è¿‡æ»¤"""
    
    def __init__(self, github_token=None):
        super().__init__("DuckDB")
        self.github_token = github_token
        self.repo = "duckdb/duckdb"
        self.api_base = f"https://api.github.com/repos/{self.repo}/issues"
        
        if github_token:
            self.session.headers['Authorization'] = f'token {github_token}'
    
    def crawl(self, max_bugs: int = 100) -> List[Dict]:
        """çˆ¬å–DuckDBçš„GitHub issues - ä¸ä½¿ç”¨æ ‡ç­¾è¿‡æ»¤"""
        print(f"\nğŸ” çˆ¬å– {self.dbms_name} (GitHub - æ— æ ‡ç­¾è¿‡æ»¤)...")
        
        page = 1
        count = 0
        
        while count < max_bugs:
            params = {
                'state': 'all',
                'per_page': 100,
                'page': page,
                'since': '2020-09-01T00:00:00Z'  # åªè¦2020å¹´9æœˆåçš„
            }
            
            print(f"  ğŸ“„ ç¬¬ {page} é¡µ...")
            
            try:
                response = self.session.get(self.api_base, params=params, timeout=30)
                
                if response.status_code == 403:
                    print("  â° APIé™æµï¼Œç­‰å¾…60ç§’...")
                    time.sleep(60)
                    continue
                
                response.raise_for_status()
                issues = response.json()
                
                if not issues:
                    break
                
                for issue in issues:
                    if count >= max_bugs:
                        break
                    
                    # è·³è¿‡PR
                    if issue.get('pull_request'):
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥åƒbug
                    if self._looks_like_bug(issue):
                        bug_data = self._parse_github_issue(issue)
                        if bug_data:
                            self.bugs.append(bug_data)
                            count += 1
                            print(f"    âœ… {bug_data['title'][:60]}...")
                
                page += 1
                time.sleep(1)
                
            except Exception as e:
                print(f"  âŒ é”™è¯¯: {e}")
                break
        
        print(f"  ğŸ‰ {self.dbms_name}: æ–°å¢ {len(self.bugs)} ä¸ªbugs")
        return self.bugs
    
    def _looks_like_bug(self, issue: Dict) -> bool:
        """åˆ¤æ–­issueæ˜¯å¦çœ‹èµ·æ¥åƒbug"""
        title = issue['title'].lower()
        body = (issue.get('body') or '').lower()
        
        # Bugç›¸å…³å…³é”®è¯
        bug_keywords = [
            'bug', 'crash', 'error', 'fail', 'incorrect', 'wrong',
            'assertion', 'segfault', 'panic', 'exception', 'issue'
        ]
        
        return any(kw in title or kw in body for kw in bug_keywords)
    
    def _parse_github_issue(self, issue: Dict) -> Optional[Dict]:
        """è§£æGitHub issue"""
        try:
            created_at = datetime.fromisoformat(issue['created_at'].replace('Z', '+00:00'))
            date_str = created_at.strftime("%d/%m/%Y")
            
            # æå–SQLæµ‹è¯•ç”¨ä¾‹
            body = issue.get('body', '') or ''
            test_cases = self._extract_sql(body)
            
            # åˆ¤æ–­oracleç±»å‹
            oracle = self._determine_oracle(issue['title'], body)
            
            bug_data = {
                "date": date_str,
                "dbms": self.dbms_name,
                "links": {
                    "bugreport": issue['html_url']
                },
                "oracle": oracle,
                "reporter": issue['user']['login'],
                "status": "open" if issue['state'] == 'open' else "fixed",
                "title": issue['title']
            }
            
            if test_cases:
                bug_data["test"] = test_cases
            
            return bug_data
            
        except Exception as e:
            return None
    
    def _extract_sql(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–SQL"""
        sql_statements = []
        
        # æå–```sqlä»£ç å—
        pattern = r'```(?:sql|SQL)\n(.*?)```'
        for match in re.finditer(pattern, text, re.DOTALL):
            code = match.group(1).strip()
            lines = [l.strip() for l in code.split('\n') if l.strip() and not l.strip().startswith('--')]
            sql_statements.extend(lines[:5])
        
        return sql_statements[:10]
    
    def _determine_oracle(self, title: str, body: str) -> str:
        """åˆ¤æ–­oracleç±»å‹"""
        text = (title + ' ' + body).lower()
        
        if any(kw in text for kw in ['crash', 'segfault', 'assertion', 'panic']):
            return "crash"
        elif any(kw in text for kw in ['wrong result', 'incorrect', 'expected']):
            return "PQS"
        elif any(kw in text for kw in ['error', 'exception', 'fail']):
            return "error"
        else:
            return "unknown"


class MySQLBugzillaCrawler(BaseBugCrawler):
    """MySQL Bugzillaçˆ¬è™«"""
    
    def __init__(self):
        super().__init__("MySQL")
        self.base_url = "https://bugs.mysql.com"
        # Bugzilla REST API
        self.api_url = f"{self.base_url}/rest.cgi/bug"
    
    def crawl(self, max_bugs: int = 100) -> List[Dict]:
        """çˆ¬å–MySQL Bugzilla"""
        print(f"\nğŸ” çˆ¬å– {self.dbms_name} (Bugzilla)...")
        
        try:
            # Bugzilla APIå‚æ•°
            params = {
                'product': 'MySQL Server',
                'limit': max_bugs,
                'order': 'bug_id DESC',  # æœ€æ–°çš„
                'include_fields': 'id,summary,status,creation_time,component'
            }
            
            print(f"  ğŸ“„ è®¿é—®Bugzilla API...")
            response = self.session.get(self.api_url, params=params, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                bugs_data = data.get('bugs', [])
                
                for bug in bugs_data[:max_bugs]:
                    bug_data = self._parse_bugzilla_bug(bug)
                    if bug_data:
                        self.bugs.append(bug_data)
                        print(f"    âœ… {bug_data['title'][:60]}...")
                
                print(f"  ğŸ‰ {self.dbms_name}: æ–°å¢ {len(self.bugs)} ä¸ªbugs")
            else:
                print(f"  âš ï¸  Bugzilla APIè¿”å›: {response.status_code}")
                print(f"  ğŸ’¡ å°è¯•å¤‡é€‰æ–¹æ¡ˆ: HTMLè§£æ...")
                self._crawl_html_fallback(max_bugs)
        
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
            print(f"  ğŸ’¡ å°è¯•å¤‡é€‰æ–¹æ¡ˆ...")
            self._crawl_html_fallback(max_bugs)
        
        return self.bugs
    
    def _parse_bugzilla_bug(self, bug: Dict) -> Optional[Dict]:
        """è§£æBugzilla bug"""
        try:
            # è½¬æ¢æ—¥æœŸ
            date_obj = datetime.fromisoformat(bug['creation_time'].replace('Z', '+00:00'))
            date_str = date_obj.strftime("%d/%m/%Y")
            
            bug_id = bug['id']
            
            return {
                "date": date_str,
                "dbms": self.dbms_name,
                "links": {
                    "bugreport": f"{self.base_url}/bug.php?id={bug_id}"
                },
                "oracle": "unknown",
                "reporter": "MySQL Team",
                "status": bug.get('status', 'unknown').lower(),
                "title": bug.get('summary', 'No title')
            }
        except Exception as e:
            return None
    
    def _crawl_html_fallback(self, max_bugs: int):
        """å¤‡é€‰æ–¹æ¡ˆï¼šHTMLè§£æ"""
        print(f"  ğŸ“„ ä½¿ç”¨HTMLè§£ææ–¹æ¡ˆ...")
        # è¿™é‡Œå¯ä»¥å®ç°HTMLè§£æé€»è¾‘
        pass


class PostgreSQLMailingListCrawler(BaseBugCrawler):
    """PostgreSQLé‚®ä»¶åˆ—è¡¨çˆ¬è™«"""
    
    def __init__(self):
        super().__init__("PostgreSQL")
        self.base_url = "https://www.postgresql.org/list/pgsql-bugs"
    
    def crawl(self, max_bugs: int = 100) -> List[Dict]:
        """çˆ¬å–PostgreSQLé‚®ä»¶åˆ—è¡¨"""
        print(f"\nğŸ” çˆ¬å– {self.dbms_name} (é‚®ä»¶åˆ—è¡¨)...")
        print(f"  âš ï¸  PostgreSQLä½¿ç”¨é‚®ä»¶åˆ—è¡¨ï¼Œè§£æè¾ƒå¤æ‚")
        print(f"  ğŸ’¡ å»ºè®®: æ‰‹åŠ¨è®¿é—® {self.base_url}")
        
        # é‚®ä»¶åˆ—è¡¨çˆ¬å–æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›æ¡†æ¶
        # å®é™…å®ç°éœ€è¦è§£æé‚®ä»¶å½’æ¡£é¡µé¢
        
        return self.bugs


class MariaDBJiraCrawler(BaseBugCrawler):
    """MariaDB Jiraçˆ¬è™«"""
    
    def __init__(self):
        super().__init__("MariaDB")
        self.base_url = "https://jira.mariadb.org"
        self.api_url = f"{self.base_url}/rest/api/2/search"
    
    def crawl(self, max_bugs: int = 100) -> List[Dict]:
        """çˆ¬å–MariaDB Jira"""
        print(f"\nğŸ” çˆ¬å– {self.dbms_name} (Jira)...")
        
        try:
            # Jira JQLæŸ¥è¯¢
            params = {
                'jql': 'project=MDEV AND type=Bug ORDER BY created DESC',
                'maxResults': max_bugs,
                'fields': 'summary,status,created,reporter'
            }
            
            print(f"  ğŸ“„ è®¿é—®Jira API...")
            response = self.session.get(self.api_url, params=params, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                issues = data.get('issues', [])
                
                for issue in issues:
                    bug_data = self._parse_jira_issue(issue)
                    if bug_data:
                        self.bugs.append(bug_data)
                        print(f"    âœ… {bug_data['title'][:60]}...")
                
                print(f"  ğŸ‰ {self.dbms_name}: æ–°å¢ {len(self.bugs)} ä¸ªbugs")
            else:
                print(f"  âš ï¸  Jira APIéœ€è¦è®¤è¯æˆ–è®¿é—®å—é™")
                print(f"  ğŸ’¡ å»ºè®®: æ‰‹åŠ¨è®¿é—® {self.base_url}")
        
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
        
        return self.bugs
    
    def _parse_jira_issue(self, issue: Dict) -> Optional[Dict]:
        """è§£æJira issue"""
        try:
            fields = issue.get('fields', {})
            
            # è½¬æ¢æ—¥æœŸ
            date_str = fields.get('created', '')
            if date_str:
                date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                date_str = date_obj.strftime("%d/%m/%Y")
            
            return {
                "date": date_str,
                "dbms": self.dbms_name,
                "links": {
                    "bugreport": f"{self.base_url}/browse/{issue['key']}"
                },
                "oracle": "unknown",
                "reporter": fields.get('reporter', {}).get('displayName', 'Unknown'),
                "status": fields.get('status', {}).get('name', 'unknown').lower(),
                "title": fields.get('summary', 'No title')
            }
        except Exception as e:
            return None


def main():
    """ä¸»å‡½æ•° - ä»å¤šä¸ªæ•°æ®æºçˆ¬å–bugs"""
    
    print("=" * 70)
    print("ğŸ•·ï¸  å¤šæºBugçˆ¬è™«ç³»ç»Ÿ")
    print("=" * 70)
    print("ğŸ“‹ å°†ä»å„æ•°æ®åº“çš„å®˜æ–¹ç½‘ç«™çˆ¬å–æœ€æ–°bugs\n")
    
    # å°è¯•è·å–GitHub Token
    github_token = None
    try:
        from crawler_config import GITHUB_TOKEN
        github_token = GITHUB_TOKEN
    except:
        pass
    
    all_bugs = []
    
    # 1. SQLite - å®˜æ–¹Bug Tracker
    print("\n" + "=" * 70)
    print("1ï¸âƒ£ SQLite (å®˜æ–¹Bug Tracker)")
    print("=" * 70)
    sqlite_crawler = SQLiteBugCrawler()
    sqlite_bugs = sqlite_crawler.crawl(max_bugs=50)
    all_bugs.extend(sqlite_bugs)
    if sqlite_bugs:
        sqlite_crawler.save_bugs("bugs_sqlite_new.json")
    
    # 2. DuckDB - GitHub (æ— æ ‡ç­¾è¿‡æ»¤)
    print("\n" + "=" * 70)
    print("2ï¸âƒ£ DuckDB (GitHub - æ”¾å®½è¿‡æ»¤)")
    print("=" * 70)
    duckdb_crawler = DuckDBGitHubCrawler(github_token)
    duckdb_bugs = duckdb_crawler.crawl(max_bugs=100)
    all_bugs.extend(duckdb_bugs)
    if duckdb_bugs:
        duckdb_crawler.save_bugs("bugs_duckdb_new.json")
    
    # 3. MySQL - Bugzilla
    print("\n" + "=" * 70)
    print("3ï¸âƒ£ MySQL (Bugzilla)")
    print("=" * 70)
    mysql_crawler = MySQLBugzillaCrawler()
    mysql_bugs = mysql_crawler.crawl(max_bugs=50)
    all_bugs.extend(mysql_bugs)
    if mysql_bugs:
        mysql_crawler.save_bugs("bugs_mysql_new.json")
    
    # 4. PostgreSQL - é‚®ä»¶åˆ—è¡¨
    print("\n" + "=" * 70)
    print("4ï¸âƒ£ PostgreSQL (é‚®ä»¶åˆ—è¡¨)")
    print("=" * 70)
    pg_crawler = PostgreSQLMailingListCrawler()
    pg_bugs = pg_crawler.crawl(max_bugs=50)
    all_bugs.extend(pg_bugs)
    
    # 5. MariaDB - Jira
    print("\n" + "=" * 70)
    print("5ï¸âƒ£ MariaDB (Jira)")
    print("=" * 70)
    mariadb_crawler = MariaDBJiraCrawler()
    mariadb_bugs = mariadb_crawler.crawl(max_bugs=50)
    all_bugs.extend(mariadb_bugs)
    if mariadb_bugs:
        mariadb_crawler.save_bugs("bugs_mariadb_new.json")
    
    # ä¿å­˜æ‰€æœ‰bugs
    if all_bugs:
        print("\n" + "=" * 70)
        print("ğŸ’¾ ä¿å­˜æ‰€æœ‰æ–°çˆ¬å–çš„bugs...")
        with open("bugs_multi_source.json", 'w', encoding='utf-8') as f:
            json.dump(all_bugs, f, indent=4, ensure_ascii=False)
        print(f"  âœ… ä¿å­˜ {len(all_bugs)} ä¸ªbugsåˆ° bugs_multi_source.json")
    
    # ç»Ÿè®¡
    from collections import Counter
    dbms_count = Counter(bug['dbms'] for bug in all_bugs)
    
    print("\n" + "=" * 70)
    print("ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡:")
    print("=" * 70)
    for dbms, count in dbms_count.most_common():
        print(f"  âœ… {dbms:15s} {count:3d} ä¸ªæ–°bugs")
    print(f"  {'â”€' * 36}")
    print(f"  æ€»è®¡:          {len(all_bugs):3d} ä¸ªæ–°bugs")
    print("=" * 70)
    
    print("\nğŸ’¡ æç¤º:")
    print("  - å„æ•°æ®åº“çš„bugså·²åˆ†åˆ«ä¿å­˜")
    print("  - æ‰€æœ‰bugsåˆå¹¶åœ¨: bugs_multi_source.json")
    print("  - ä½¿ç”¨merge_multi_db.pyåˆå¹¶åˆ°ä¸»æ–‡ä»¶")


if __name__ == "__main__":
    # å®‰è£…ä¾èµ–æç¤º
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾èµ–: BeautifulSoup4")
        print("ğŸ“¦ å®‰è£…å‘½ä»¤: pip install beautifulsoup4")
        exit(1)
    
    main()

